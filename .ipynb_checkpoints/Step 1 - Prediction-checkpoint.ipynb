{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d986548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import the usual suspects\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import Toolbox as tb\n",
    "import Lossfunction as lf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c028ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize warning log container \n",
    "log = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457688a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before downcast: 0.502 GB and float64    72\n",
      "int64      68\n",
      "dtype: int64\n",
      "After downcast: 0.161 GB and float32    72\n",
      "int8       68\n",
      "dtype: int64\n",
      "Before downcast: 0.185 GB and float64    72\n",
      "int64      68\n",
      "dtype: int64\n",
      "After downcast: 0.059 GB and float32    72\n",
      "int8       68\n",
      "dtype: int64\n",
      "Before downcast: 2.625 GB and float64    72\n",
      "int64      68\n",
      "dtype: int64\n",
      "After downcast: 0.841 GB and float32    72\n",
      "int8       68\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "training = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\Data' +'\\\\data_t_01.csv', index_col = [\"permno\", \"date\"])\n",
    "validation = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\Data' +'\\\\data_v_01.csv', index_col = [\"permno\", \"date\"])\n",
    "test = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\Data' +'\\\\data_tt_01.csv', index_col = [\"permno\", \"date\"])\n",
    "\n",
    "# Downcast\n",
    "tb.downcast(training)\n",
    "tb.downcast(validation)\n",
    "tb.downcast(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "835de445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "assert np.all(training.columns == validation.columns)\n",
    "assert np.all(training.columns == test.columns)\n",
    "assert np.all(validation.columns == test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b73706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Y and X\n",
    "training_x = training.iloc[:, :-1]\n",
    "training_y = training.ret\n",
    "\n",
    "validation_x = validation.iloc[:, :-1]\n",
    "validation_y = validation.ret\n",
    "\n",
    "test_x = test.iloc[:, :-1]\n",
    "test_y = test.ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "055fc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 1: Simple Linear (PCR in Gu, kelly, and Xiu (2020) due to PCA)\n",
    "\n",
    "# Fit model on training set\n",
    "linear_reg = lr().fit(training_x, training_y)\n",
    "\n",
    "# Predict model on test set\n",
    "linear_reg_pred = linear_reg.predict(test_x)\n",
    "\n",
    "# Loss function\n",
    "loss_linear_reg = lf.loss_function(linear_reg_pred, test_y)\n",
    "\n",
    "# Explained variation\n",
    "xplained_var_linear_reg = lf.explained_variation(linear_reg_pred, test_y)\n",
    "\n",
    "# Portfolio sorts\n",
    "\n",
    "# Stock characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2a101c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.87910449429597"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_linear_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "32b6cfc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01487567])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xplained_var_linear_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d24526ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2: Lasso \n",
    "\n",
    "# Fit model on training set and select tuning parameter based on validation set\n",
    "lambda_grid = lf.lambda_grid(training_x, training_y)\n",
    "loss_validation = []\n",
    "\n",
    "for lamb in lambda_grid:\n",
    "    lasso = Lasso(alpha = lamb).fit(training_x, training_y)\n",
    "    lasso_pred = lasso.predict(validation_x)\n",
    "    loss_validation.append(lf.loss_function(lasso_pred, validation_y))\n",
    "\n",
    "# Fit model with squared euclidian distance minimizing tuning parameter as chosen by forward chaining\n",
    "lambda_min = lambda_grid[loss_validation.index(min(loss_validation))]\n",
    "lasso_min = Lasso(alpha = lambda_min).fit(training_x, training_y)\n",
    "lasso_min_pred = lasso_min.predict(test_x)\n",
    "\n",
    "# Loss function\n",
    "loss = lf.loss_function(lasso_min_pred, test_y)\n",
    "\n",
    "# Explaiend variation\n",
    "xplained_var_lasso = lf.explained_variation(lasso_min_pred, test_y)\n",
    "\n",
    "# Portfolio sorts\n",
    "\n",
    "# Stock characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c6db860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c431da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gu, Kelly Xiu (2020) NN:\n",
    "'''\n",
    "\n",
    "* Gu, Kelly og Xiu laver ikke hyperparameter tuning som sådan. Det er for computationally intensive hvorfor de i stedet\n",
    "selecter et par NN architectures ex-ante og bruger dette som et reasonable estimate for lower bound performance for NN. \n",
    "\n",
    "* De bruger i stedet validation set til deres early stopping algo; efter hver epoche laver vi prediction på validation set\n",
    "hvis de begynder at stige, så stopper vi algoen - ideen er her at der begynder at blive overfittet. i model.fit() er der \n",
    "et argument der hedder validation_data, hvor man kan specificere validation sæt. Video, hvor de gør det: \n",
    "https://www.youtube.com/watch?v=qFJeN9V1ZsI&t=34s&ab_channel=freeCodeCamp.org\n",
    "\n",
    "* Kan ikke umiddelbart se hvor mange epoches de bruger ej heller hvad deres batch-size er. Ej heller hvad learning rate\n",
    "starter med at være og hvad regularization parameter er. \n",
    "\n",
    "\n",
    "To-Do:\n",
    "* Skal jeg lære at lave NN på den advanced måde også? \n",
    "* Objective function skal være regularized\n",
    "* ReLu giver kun positve Y'er; skal kunne være negative også. Tror løsningen kunne være batch normalization.\n",
    "* De øvrige ting er mindre vigtige, men det vil være learning rate shrinkage, early stopping (for at bruge validation sæt) \n",
    "og ensemble \n",
    "* Indirekte fikse data partitioning, så jeg kan lave predictions for hvert split \n",
    "* Skal jeg lave de samme NN arkitekturer, som de gør? Eller skal jeg bare nøjes med en? \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84fb5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 3: Neural Network\n",
    "\n",
    "# Build NN architecture \n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(Dense(units = 32, activation = 'relu', input_dim = len(training_x.columns))) # 1. Hidden layer\n",
    "model.add(Dense(units = 16, activation = 'relu')) # 2. Hidden layer\n",
    "model.add(Dense(units = 8, activation = 'relu')) # 3. Hidden layer\n",
    "model.add(Dense(units = 4, activation = 'relu')) # 4. Hidden layer\n",
    "model.add(Dense(units = 1, activation = 'linear')) # Output layer - linear fixes nonnegative predictions\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error') # jeg kan også bruge huber loss ved 'huber'\n",
    "\n",
    "# Fit model\n",
    "model.fit(training_x, training_y, epochs = 1, batch_size = 40, verbose = 1)\n",
    "\n",
    "# Compute predictions\n",
    "y_hat = model.predict(test_x)\n",
    "\n",
    "# Compute loss\n",
    "lf.loss_function(y_hat, test_y)\n",
    "\n",
    "# Variation explained\n",
    "lf.explained_variation(y_hat, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e28cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algo 4 + 5: Gradient Boosting + Random Forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15e0295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To code:\n",
    "# Skal jeg lave en pipeline for alle mine ML modeller som ham der kodede til den der konkurrence? \n",
    "# Skal have lavet en mappe til mine scripts\n",
    "# Skal have relateret prediction til stocks så jeg kan se hvilke der er høje og lave\n",
    "    # Her kunne jeg have et problem med et tale om macroøkonomisk predictors. Men det er vel også mere firm char\n",
    "    # som er interessant at vide \n",
    "# Skal have kodet portfolio sorts -- her skal jeg måske kigge lidt på aflevering i AME2\n",
    "    # 1) Ranger i deciler efter predicted return. 2) Lav porteføljer hver periode baseret på prediction. \n",
    "    # 3) Se hvad return er for hver portefølje hver periode og tag average over perioden \n",
    "# Kode noget op som samler resultaterne og kommer dem i en latex tabel. f.eks. explained variation og MSE\n",
    "    \n",
    "# Algoritmer: OLS, Lasso / EN, NN, RF (classification?), GRBT "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
