{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b4ea73",
   "metadata": {},
   "source": [
    "## Master's Thesis - Machine Learning in Asset Pricing\n",
    "\n",
    "### Thomas Theodor Kjølbye \n",
    "\n",
    "The Following script handles all data used in the paper. On my computer, the entire script takes approximately 5 minutes to run. The data consist of individual firm characteristics as well as macroeconomic variables and are generously made available by Professors Gu, Kelly, Xiu, and Goyal. \n",
    "\n",
    "The script is the 2nd of two data processing scripts. The first one does not compute interaction terms or reduce dimensionality by PCA. However, it does perform the rank normalization leveraged in Gu, Kelly, and Xiu (2020). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a466df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the usual suspects\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import Toolbox as tb\n",
    "import time as time\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f529d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize warning log container \n",
    "log = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ebb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a2695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The firm characteristics dataset is 2.939 GB\n"
     ]
    }
   ],
   "source": [
    "# Load monthly returns data from CRSP and process data\n",
    "returns = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\RET.txt')\n",
    "returns.columns = returns.columns.str.strip()\n",
    "returns.columns = returns.columns.str.lower() \n",
    "returns.rename(columns = {\"col1\":\"date\"}, inplace = True)\n",
    "returns[\"date\"] = returns[\"date\"].floordiv(100)\n",
    "returns_raw = returns[[\"date\", \"permno\", \"ret\"]]\n",
    "\n",
    "# Load macroeconomic predictors and clean it\n",
    "macro_data_raw = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\macro_data.csv') \n",
    "macro_data_raw = macro_data_raw[(macro_data_raw[\"date\"] > 195612) & (macro_data_raw[\"date\"] < 201701)]\n",
    "\n",
    "# Load firm characteristics\n",
    "firm_data_raw = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\datashare.zip')\n",
    "firm_data_raw.columns = firm_data_raw.columns.str.lower()\n",
    "firm_data_raw[\"date\"] = firm_data_raw[\"date\"].floordiv(100) # https://stackoverflow.com/questions/33034559/how-to-remove-last-the-two-digits-in-a-column-that-is-of-integer-type\n",
    "firm_data_raw.set_index([\"permno\", \"date\"], inplace = True)\n",
    "print(\"The firm characteristics dataset is {:1.3f} GB\".format(firm_data_raw.memory_usage().sum()/(1024 ** 3)))\n",
    "\n",
    "# I have refrained from saving the firm char. in my wd (repo) because I am unable to push 3 GB worth of data to the github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f5917a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before downcast: 1.481 GB and float32    95\n",
      "dtype: int64\n",
      "After downcast: 1.481 GB and float32    95\n",
      "dtype: int64\n",
      "Before downcast: 0.045 GB and int32      2\n",
      "float32    1\n",
      "dtype: int64\n",
      "After downcast: 0.045 GB and int32      2\n",
      "float32    1\n",
      "dtype: int64\n",
      "Before downcast: 0.000 GB and float32    8\n",
      "int32      1\n",
      "dtype: int64\n",
      "After downcast: 0.000 GB and float32    8\n",
      "int32      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Downcast from 64bit flots and ints to 32bit\n",
    "tb.downcast(firm_data_raw)\n",
    "tb.downcast(returns_raw)\n",
    "tb.downcast(macro_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8244d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with returns data and filter out rows missing in returns data\n",
    "data = firm_data_raw.reset_index().merge(returns_raw, on = [\"permno\", \"date\"], how = \"inner\")\n",
    "\n",
    "# Merge with macro data \n",
    "data = data.merge(macro_data_raw, on = \"date\")\n",
    "data = data.set_index([\"permno\", \"date\"]) # 3760315 x 104 (94 char, industry dummy, returns, 8 macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "769646a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data up in (firm characteristics, macro), returns and industry codes ((Not) used in computation of inter terms)\n",
    "\n",
    "# (Firm characteristics, macro)\n",
    "non_interaction_to_drop = [\"ret\", \"sic2\"]\n",
    "interaction_data = data.drop(non_interaction_to_drop, axis = 1).reset_index()\n",
    "\n",
    "# Returns\n",
    "returns = data.ret\n",
    "\n",
    "# Industry codes\n",
    "industry_code = data.sic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49dca326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save interaction data\n",
    "interaction_data.to_csv(os.path.dirname(os.getcwd()) + '\\\\interaction_data.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a1894eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before downcast: 2.914 GB and float64    102\n",
      "int64        2\n",
      "dtype: int64\n",
      "After downcast: 1.457 GB and float32    102\n",
      "int32        2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load data for interaction terms\n",
    "interaction_data = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\interaction_data.csv')\n",
    "tb.downcast(interaction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9c4beea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\OneDrive - Københavns Universitet\\Documents\\Økonomi - Kandidat\\6. Semester\\Speciale\\Masters-Thesis\\Toolbox.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"constant\"] = 1\n"
     ]
    }
   ],
   "source": [
    "# Split and clean data\n",
    "tfirm, vfirm, ttfirm, tmacro, vmacro, ttmacro = tb.data_processing(data = interaction_data, start = 197501, end = 198612)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f455c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute interaction terms for training and validation\n",
    "tinteraction, mean, std = tb.interaction(tfirm, tmacro)\n",
    "#vinteraction, _, _ = tb.interaction(vfirm, vmacro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77e8a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute interaction terms for testing \n",
    "tb.interaction_noRAM(ttfirm, ttmacro, mean = mean, std = std, filename = 'test_interaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38cbdbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for training data\n",
    "pca = PCA(n_components = 0.95)\n",
    "pca.fit(tinteraction)\n",
    "tdata = pca.transform(tinteraction) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cac1e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for test data\n",
    "tb.save_txt(name = os.path.dirname(os.getcwd()) + \"\\\\\" + 'test_interaction.csv', newfilename = 'test_data.csv',\n",
    "                    pc = pca.components_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e255361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5249649211764336"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata.nbytes / 1024 ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "55cf7fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479317, 85)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfirm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dadd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute interaction terms for training data \n",
    "\n",
    "# Initialize empty dataframe\n",
    "interaction_training = pd.DataFrame(columns = range(0), index = range(firm_training.shape[0]))\n",
    "\n",
    "# Loop through macro predictors, multiply with characteristics and return to interaction dataframe\n",
    "for count, value in enumerate(macro_training.columns):\n",
    "                          \n",
    "    macro_ite = macro_training[value].values # .values returns np.array not pd.series\n",
    "    product_ite = macro_ite.reshape(-1,1) * firm_training.values # 2D to make compatible for element-wise multiplication       \n",
    "    column_ite = [str(col) + f'X{value}' for col in firm_training.columns] \n",
    "    df_ite = pd.DataFrame(product_ite, columns = column_ite)\n",
    "    interaction_training = pd.concat([interaction_training, df_ite], axis = 1)\n",
    "    \n",
    "    #interaction_training[column_ite] = product_ite \n",
    "    #interaction.to_csv(os.path.dirname(os.getcwd()) + f'\\\\firmX{value}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f51853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training data for standardization and PCA \n",
    "training_data = pd.concat([firm_training.reset_index(), interaction_training], axis = 1)\n",
    "training_data = training_data.set_index([\"permno\",\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b35df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "training_data = training_data.apply(lambda x: (x - np.mean(x)) / np.std(x), axis = 0)\n",
    "training_data = training_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51174de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA \n",
    "pca = PCA(n_components = 0.95)\n",
    "pca.fit(training_data)\n",
    "reduced = pca.transform(training_data) \n",
    "\n",
    "\n",
    "# pca.explained_variance_ giver eigenvalues for hver principal component\n",
    "# reduced.shape giver antal covariates i reduced datasæt\n",
    "# reduced.nbytes / 1024 ** 3 giver size i GB for reduced data sæt som er numpy array. Det er 0.262482 GB\n",
    "'''\n",
    "Manuel måde at få eigenvalues på:\n",
    "training_data_tester = training_data.copy()\n",
    "n_samples = reduced.shape[0]\n",
    "# We center the data and compute the sample covariance matrix.\n",
    "training_data_tester -= np.mean(training_data_tester, axis=0)\n",
    "cov_matrix = np.dot(training_data_tester.T, training_data_tester) / n_samples\n",
    "for eigenvector in pca.components_:\n",
    "    print(np.dot(eigenvector.T, np.dot(cov_matrix, eigenvector)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a44ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute interaction terms for the test data - jeg gør det på træning men metoden bliver det samme til funktionen\n",
    "macro_training_test = macro_training.copy()\n",
    "macro_training_test.insert(0, \"constant\", 1)\n",
    "\n",
    "for i in range(firm_training.shape[0]):\n",
    "    \n",
    "    firm_row = firm_training.iloc[i, :].values.reshape(-1,1).T # 1x88\n",
    "    macro_row = macro_training_test.iloc[i, :].values.reshape(-1,1) # 9x1 \n",
    "    \n",
    "    interaction = macro_row @ firm_row # 9x1 @ 1x88 = 9x88\n",
    "    interaction_flat = interaction.reshape((1, training_data.shape[1]))\n",
    "    interaction_std = (interaction_flat - training_mean) / training_std # Elementwise\n",
    "    interaction_std = np.nan_to_num(interaction_std)\n",
    "    \n",
    "    to_append = (interaction_std[0]).tolist()\n",
    "    #to_append = (interaction.reshape((1, 792))[0]).tolist() # https://stackoverflow.com/questions/39694318/difference-between-single-and-double-bracket-numpy-array\n",
    "    \n",
    "    with open(\"test.csv\", \"a\", newline = \"\") as t:\n",
    "\n",
    "        writer = csv.writer(t)\n",
    "        writer.writerow(to_append)\n",
    "\n",
    "# Husk at gem andet sted end i repo! \n",
    "\n",
    "# Der kommer til at være en lille ændring ift. koden i funktionen. øverst vælger jeg macro_training_test, det kommer til at\n",
    "# være en del af det samlede træningssæt som jeg smider ind i funktionen. jeg kommer altså ikke til at have separate data\n",
    "# sæt som ejg smider ind i ufnktionen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denne her skal køre inden jeg standardiserer training_data - den skal komme tidligere når jeg\n",
    "# laver funktionen (efter alle 3 sæt har fået lavet interatkion terms)\n",
    "training_mean = training_data.mean().values.reshape(-1,1).T\n",
    "training_std = training_data.std().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a110790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"test.csv\", \"r\", newline = \"\") as t:\n",
    "    for line in range(5):\n",
    "        line = t.readline()\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadtxt(name):\n",
    "    with open(name, \"r\") as file:\n",
    "        for line in file:\n",
    "            line_int = np.fromstring(line, sep = \",\").reshape(-1,1).T # 1xK 2D array\n",
    "            yield line_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a62349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_each_line(name, pc): # name er ikke nødvendig som den er nu \n",
    "    \n",
    "    for line in loadtxt(name):\n",
    "        pca_line = np.dot(line, pc)\n",
    "        to_append = (pca_line[0]).tolist()\n",
    "        \n",
    "        yield to_append\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01511e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_txt(name, newfilename, pc):\n",
    "    with open(newfilename, \"a\", newline = \"\") as newfile:\n",
    "        writer = csv.writer(newfile)\n",
    "        for newline in pca_each_line(name, pc):\n",
    "            writer.writerow(newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccdf286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeg har fundet ud af at jeg skal lave PCA på hvert træningsdatasæt og på hvert validation sæt og test!.\n",
    "\n",
    "# Jeg skal faktisk lave al databehandling jeg har gjort her for hvert fold fordi træning og validation vil være anderledes\n",
    "# for hver fold -- hvad er en smart måde at gøre det på? Lave en funktion som klarer alt data behandling og som bare \n",
    "# spytter træning og validation sæt ud? \n",
    "\n",
    "# Lav PCA på træningssæt så kan jeg lave funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1875f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Industry dummies\n",
    "# Merge\n",
    "# DONE med træningssæt "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
