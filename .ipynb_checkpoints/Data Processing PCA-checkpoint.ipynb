{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b4ea73",
   "metadata": {},
   "source": [
    "## Master's Thesis - Machine Learning in Asset Pricing\n",
    "\n",
    "### Thomas Theodor Kjølbye \n",
    "\n",
    "The Following script handles all data used in the paper. On my computer, the entire script takes approximately 5 minutes to run. The data consist of individual firm characteristics as well as macroeconomic variables and are generously made available by Professors Gu, Kelly, Xiu, and Goyal. \n",
    "\n",
    "The script is the 2nd of two data processing scripts. The first one does not compute interaction terms or reduce dimensionality by PCA. However, it does perform the rank normalization leveraged in Gu, Kelly, and Xiu (2020). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a466df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the usual suspects\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import Toolbox as tb\n",
    "import time as time\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f529d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize warning log container \n",
    "log = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6ebb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\thoma\\\\OneDrive - Københavns Universitet\\\\Documents\\\\Økonomi - Kandidat\\\\6. Semester\\\\Speciale\\\\Masters-Thesis'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a2695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The firm characteristics dataset is 2.939 GB\n"
     ]
    }
   ],
   "source": [
    "# Load monthly returns data from CRSP and process data\n",
    "returns = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\RET.txt')\n",
    "returns.columns = returns.columns.str.strip()\n",
    "returns.columns = returns.columns.str.lower() \n",
    "returns.rename(columns = {\"col1\":\"date\"}, inplace = True)\n",
    "returns[\"date\"] = returns[\"date\"].floordiv(100)\n",
    "returns_raw = returns[[\"date\", \"permno\", \"ret\"]]\n",
    "\n",
    "# Load macroeconomic predictors and clean it\n",
    "macro_data_raw = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\macro_data.csv') \n",
    "macro_data_raw = macro_data_raw[(macro_data_raw[\"date\"] > 195612) & (macro_data_raw[\"date\"] < 201701)]\n",
    "macro_data_raw[\"constant\"] = 1 # Add column of ones for interaction terms later\n",
    "\n",
    "# Load firm characteristics\n",
    "firm_data_raw = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\datashare.zip')\n",
    "firm_data_raw.columns = firm_data_raw.columns.str.lower()\n",
    "firm_data_raw[\"date\"] = firm_data_raw[\"date\"].floordiv(100) # https://stackoverflow.com/questions/33034559/how-to-remove-last-the-two-digits-in-a-column-that-is-of-integer-type\n",
    "firm_data_raw.set_index([\"permno\", \"date\"], inplace = True)\n",
    "print(\"The firm characteristics dataset is {:1.3f} GB\".format(firm_data_raw.memory_usage().sum()/(1024 ** 3)))\n",
    "\n",
    "# I have refrained from saving the firm char. in my wd (repo) because I am unable to push 3 GB worth of data to the github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f5917a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before downcast: 1.481 GB and float32    95\n",
      "dtype: int64\n",
      "After downcast: 1.481 GB and float32    95\n",
      "dtype: int64\n",
      "Before downcast: 0.045 GB and int32      2\n",
      "float32    1\n",
      "dtype: int64\n",
      "After downcast: 0.045 GB and int32      2\n",
      "float32    1\n",
      "dtype: int64\n",
      "Before downcast: 0.000 GB and float32    8\n",
      "int32      1\n",
      "int8       1\n",
      "dtype: int64\n",
      "After downcast: 0.000 GB and float32    8\n",
      "int32      1\n",
      "int8       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Downcast from 64bit flots and ints to 32bit\n",
    "tb.downcast(firm_data_raw)\n",
    "tb.downcast(returns_raw)\n",
    "tb.downcast(macro_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8244d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows missing in returns data by merging\n",
    "data = firm_data_raw.reset_index().merge(returns_raw, on = [\"permno\", \"date\"], how = \"inner\")\n",
    "\n",
    "# Scale macroeconomic data to same shape as other data by merging\n",
    "data = data.merge(macro_data_raw, on = \"date\")\n",
    "data = data.set_index([\"permno\", \"date\"]) # 3760315 x 105 (94 char, industry dummy, returns, 8 macro, ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "769646a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dataset in 1) firm chars + macro (FM), 2) returns, and 3) industry codes\n",
    "\n",
    "# 1)\n",
    "FM_todrop = [\"ret\", \"sic2\"]\n",
    "FM_data = data.drop(FM_todrop, axis = 1).reset_index()\n",
    "\n",
    "# 2) \n",
    "returns = data.ret\n",
    "\n",
    "# 3) \n",
    "industry_code = data.sic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e39bdabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FM data data\n",
    "FM_data.to_csv(os.path.dirname(os.getcwd()) + '\\\\FM_data.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2c6dede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before downcast: 2.942 GB and float64    102\n",
      "int64        3\n",
      "dtype: int64\n",
      "After downcast: 1.460 GB and float32    102\n",
      "int32        2\n",
      "int8         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### NOTE: Above code can be ignored once FM_data.csv has been saved to disc\n",
    "\n",
    "# Load FM data\n",
    "FM_data = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\FM_data.csv')\n",
    "tb.downcast(FM_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58e8fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates partitioning the training, validation, and test set\n",
    "tv_dates = [197501, 197601, 197701, 197801, 197901, 198001, 198101, 198201, 198301, 198401, 198501, 198601]\n",
    "v_dates = [197601, 197701, 197801, 197901, 198001, 198101, 198201, 198301, 198401, 198501, 198601, 198701]\n",
    "t_dates = [198701, 198701, 198701, 198701, 198701, 198701, 198701, 198701, 198701, 198701, 198701, 198701]\n",
    "\n",
    "tv_dates = tv_dates[1]\n",
    "v_dates = v_dates[1]\n",
    "t_dates = t_dates[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9c4beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and clean data according to selected dates\n",
    "tfirm, vfirm, ttfirm, tmacro, vmacro, ttmacro = tb.data_processing(data = FM_data, TV_date = tv_dates, \n",
    "                                                                   V_date = v_dates, T_date = t_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f455c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute interaction terms for training and validation data\n",
    "interaction_t, mean, std = tb.interaction(tfirm, tmacro)\n",
    "interaction_v, _, _ = tb.interaction(vfirm, vmacro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3a5b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute interaction terms for test data\n",
    "tb.interaction_noRAM(ttfirm, ttmacro, mean = mean, std = std, filename = 'interaction_tt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48d4d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA: Training data\n",
    "pca = PCA(n_components = 0.95)\n",
    "pca.fit(interaction_t)\n",
    "pca_data_t = pca.transform(interaction_t) \n",
    "pca_data_t = pd.DataFrame(pca_data_t)\n",
    "\n",
    "\n",
    "# PCA: Validation data\n",
    "pca_data_v = pca.transform(interaction_v)\n",
    "pca_data_v = pd.DataFrame(pca_data_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15c98d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA: Test data\n",
    "tb.save_txt(name = os.path.dirname(os.getcwd()) + \"\\\\\" + 'interaction_tt.csv', newfilename = 'pca_data_tt.csv',\n",
    "                    pc = pca.components_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39505698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before downcast: 2.746 GB and float64    147\n",
      "dtype: int64\n",
      "After downcast: 1.373 GB and float32    147\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load test data post PCA\n",
    "pca_data_tt = pd.read_csv(os.path.dirname(os.getcwd()) + \"\\\\pca_data_tt.csv\", header = None)\n",
    "tb.downcast(pca_data_tt) # 2.753 GB, 1.377 GB efter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8adf1564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep industry codes for merge with remainder of predictors (covariates)\n",
    "industry_dummies_t, industry_dummies_v, industry_dummies_tt = tb.dummies(data = industry_code, TV_date = tv_dates, \n",
    "                                                                         V_date = v_dates, T_date = t_dates)\n",
    "\n",
    "# Prep returns data for merge with predictors (covariates)\n",
    "returns_t = returns[returns.index.get_level_values(\"date\") < tv_dates].reset_index()\n",
    "returns_v = returns[(returns.index.get_level_values(\"date\") >= tv_dates) & \n",
    "                                                  (returns.index.get_level_values(\"date\") < v_dates)].reset_index()\n",
    "returns_tt = returns[returns.index.get_level_values(\"date\") >= t_dates].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd4b571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data and save to disc\n",
    "\n",
    "# Training\n",
    "data_t = pd.concat([pca_data_t, industry_dummies_t], axis = 1)\n",
    "data_t = data_t.merge(returns_t, on = [\"permno\", \"date\"], how = \"inner\").set_index([\"permno\", \"date\"])\n",
    "data_t.to_csv(os.path.dirname(os.getcwd()) + '\\\\Data' +'\\\\data_t_01.csv', header = False, index = True)\n",
    "\n",
    "# Validation\n",
    "data_v = pd.concat([pca_data_v, industry_dummies_v], axis = 1)\n",
    "data_v = data_v.merge(returns_v, on = [\"permno\", \"date\"], how = \"inner\").set_index([\"permno\", \"date\"])\n",
    "data_v.to_csv(os.path.dirname(os.getcwd()) + '\\\\Data' +'\\\\data_v_01.csv', header = False, index = True)\n",
    "\n",
    "# Test\n",
    "data_tt = pd.concat([pca_data_tt, industry_dummies_tt], axis = 1)\n",
    "data_tt = data_tt.merge(returns_tt, on = [\"permno\", \"date\"], how = \"inner\").set_index([\"permno\", \"date\"])\n",
    "data_tt.to_csv(os.path.dirname(os.getcwd()) + '\\\\Data' +'\\\\data_tt_01.csv', header = False, index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
