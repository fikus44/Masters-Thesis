{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8de3de21-6556-4f1e-80e2-80a89e701949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load dependencies\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import itertools as it\n",
    "import DataProcessFunctions as DP\n",
    "import PredictionStep1 as pred\n",
    "import SupportFunctions as supp\n",
    "import LinearModelEstimation as lm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import statsmodels.api as sm  \n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import callbacks\n",
    "from matplotlib.pyplot import cm\n",
    "import time as time \n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Disable scientific notation (e) in numpy\n",
    "# Disable futurewarnings\n",
    "np.set_printoptions(suppress=True)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8f707-f8b3-4490-85d4-9463d4525581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize warning log container \n",
    "log = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76be5616-1015-4f3d-b1bc-de5deed9e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before downcast: 1.769 GB and float64    102\n",
      "int64        3\n",
      "dtype: int64\n",
      "After downcast: 0.878 GB and float32    102\n",
      "int32        2\n",
      "int8         1\n",
      "dtype: int64\n",
      "Before downcast: 0.026 GB and float64    1\n",
      "dtype: int64\n",
      "After downcast: 0.018 GB and float32    1\n",
      "dtype: int64\n",
      "Before downcast: 0.026 GB and float64    1\n",
      "dtype: int64\n",
      "After downcast: 0.018 GB and float32    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load Pre-processed data from Data Processing PCA.ipynb\n",
    "FM_data = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\FM2_data.csv')\n",
    "returns = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\returns2_data.csv').set_index([\"permno\", \"date\"])\n",
    "industry_code = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\industry2_codes.csv').set_index([\"permno\", \"date\"])\n",
    "\n",
    "# Load excess SP500 log returns data \n",
    "SP500 = pd.read_excel(os.path.dirname(os.getcwd()) + '\\\\SP500.xlsx')[\"Cum return\"]\n",
    "\n",
    "supp.downcast(FM_data)\n",
    "supp.downcast(returns)\n",
    "supp.downcast(industry_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa87af2c-5685-4f1c-a59f-1cbcb4e35d77",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data_processing_new time: 0.1698844353357951\n",
      "interaction_new time: 5.990009347597758 Training Dtypes: float32    765\n",
      "dtype: int64, Validation Dtypes: float32    765\n",
      "dtype: int64, Test Dtypes: float32    765\n",
      "dtype: int64,\n",
      "pca time: 12.85541937748591\n",
      "Rows Training set: 406519, Row Validation set: 629972, Rows test set: 63913, Columns: 168, Iteration finished: 0, Time: 19.107210187117257, \n",
      "1998/1998 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.2663463234901428\n",
      "interaction_new time: 6.647480694452922 Training Dtypes: float32    792\n",
      "dtype: int64, Validation Dtypes: float32    792\n",
      "dtype: int64, Test Dtypes: float32    792\n",
      "dtype: int64,\n",
      "pca time: 15.599776005744934\n",
      "Rows Training set: 444575, Row Validation set: 644834, Rows test set: 63116, Columns: 166, Iteration finished: 1, Time: 22.610529299577077, \n",
      "1973/1973 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.2807585120201111\n",
      "interaction_new time: 6.691159399350484 Training Dtypes: float32    792\n",
      "dtype: int64, Validation Dtypes: float32    792\n",
      "dtype: int64, Test Dtypes: float32    792\n",
      "dtype: int64,\n",
      "pca time: 17.043780088424683\n",
      "Rows Training set: 482843, Row Validation set: 658706, Rows test set: 59846, Columns: 164, Iteration finished: 2, Time: 24.11758999824524, \n",
      "1871/1871 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.3278684695561727\n",
      "interaction_new time: 6.955878965059916 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 18.840961746374767\n",
      "Rows Training set: 521296, Row Validation set: 669087, Rows test set: 57942, Columns: 164, Iteration finished: 3, Time: 26.232347456614175, \n",
      "1811/1811 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.3546460429827372\n",
      "interaction_new time: 7.306299634774526 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 20.266568144162495\n",
      "Rows Training set: 558106, Row Validation set: 678900, Rows test set: 56180, Columns: 163, Iteration finished: 4, Time: 28.045214955012003, \n",
      "1756/1756 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.3896034638086955\n",
      "interaction_new time: 7.764351371924082 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 21.71811068058014\n",
      "Rows Training set: 594100, Row Validation set: 687598, Rows test set: 56596, Columns: 168, Iteration finished: 5, Time: 29.988979335625967, \n",
      "1769/1769 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.4133721629778544\n",
      "interaction_new time: 8.175669511159262 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 22.895656184355417\n",
      "Rows Training set: 626310, Row Validation set: 696363, Rows test set: 58663, Columns: 171, Iteration finished: 6, Time: 31.61608060201009, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.488e+00, tolerance: 1.362e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1834/1834 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.49364424546559654\n",
      "interaction_new time: 7.895963776111603 Training Dtypes: float32    819\n",
      "dtype: int64, Validation Dtypes: float32    819\n",
      "dtype: int64, Test Dtypes: float32    819\n",
      "dtype: int64,\n",
      "pca time: 25.11747206846873\n",
      "Rows Training set: 656279, Row Validation set: 703774, Rows test set: 63861, Columns: 178, Iteration finished: 7, Time: 33.639507647355394, \n",
      "1996/1996 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.4854767441749573\n",
      "interaction_new time: 8.536248632272084 Training Dtypes: float32    819\n",
      "dtype: int64, Validation Dtypes: float32    819\n",
      "dtype: int64, Test Dtypes: float32    819\n",
      "dtype: int64,\n",
      "pca time: 26.305405843257905\n",
      "Rows Training set: 687364, Row Validation set: 714836, Rows test set: 65068, Columns: 176, Iteration finished: 8, Time: 35.47032494942347, \n",
      "2034/2034 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.5125718871752422\n",
      "interaction_new time: 8.698393615086873 Training Dtypes: float32    819\n",
      "dtype: int64, Validation Dtypes: float32    819\n",
      "dtype: int64, Test Dtypes: float32    819\n",
      "dtype: int64,\n",
      "pca time: 27.68926835457484\n",
      "Rows Training set: 719916, Row Validation set: 725226, Rows test set: 68286, Columns: 172, Iteration finished: 9, Time: 37.03482728799184, \n",
      "2134/2134 [==============================] - 5s 2ms/step\n",
      " data_processing_new time: 0.5486350337664286\n",
      "interaction_new time: 8.741542410850524 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 28.44377981821696\n",
      "Rows Training set: 757867, Row Validation set: 733249, Rows test set: 70136, Columns: 173, Iteration finished: 10, Time: 37.87200738588969, \n",
      "2192/2192 [==============================] - 5s 2ms/step\n",
      " data_processing_new time: 0.5752736687660217\n",
      "interaction_new time: 9.37793471813202 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 29.979388193289438\n",
      "Rows Training set: 795114, Row Validation set: 743801, Rows test set: 66870, Columns: 180, Iteration finished: 11, Time: 40.086605592568716, \n",
      "2090/2090 [==============================] - 5s 2ms/step\n",
      " data_processing_new time: 0.6076516350110371\n",
      "interaction_new time: 9.852616687615713 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 31.614731192588806\n",
      "Rows Training set: 833290, Row Validation set: 750477, Rows test set: 60191, Columns: 181, Iteration finished: 12, Time: 42.225399557749434, \n",
      "1881/1881 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.628267236550649\n",
      "interaction_new time: 10.138810698191325 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 33.14281661113103\n",
      "Rows Training set: 874593, Row Validation set: 746755, Rows test set: 56328, Columns: 193, Iteration finished: 13, Time: 44.07124530474345, \n",
      "1761/1761 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.6564720431963603\n",
      "interaction_new time: 10.691276411215464 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 34.977023188273115\n",
      "Rows Training set: 914136, Row Validation set: 739967, Rows test set: 48683, Columns: 193, Iteration finished: 14, Time: 46.49255556662877, \n",
      "1522/1522 [==============================] - 3s 2ms/step\n",
      " data_processing_new time: 0.684402871131897\n",
      "interaction_new time: 11.161251139640807 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 36.22653006315231\n",
      "Rows Training set: 949773, Row Validation set: 728804, Rows test set: 41635, Columns: 196, Iteration finished: 15, Time: 48.23654029766718, \n",
      "1302/1302 [==============================] - 3s 2ms/step\n",
      " data_processing_new time: 0.7243576208750407\n",
      "interaction_new time: 11.217506992816926 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 37.58670822779337\n",
      "Rows Training set: 982084, Row Validation set: 712497, Rows test set: 36376, Columns: 195, Iteration finished: 16, Time: 49.70722665786743, \n",
      "1137/1137 [==============================] - 3s 2ms/step\n",
      " data_processing_new time: 0.718603257338206\n",
      "interaction_new time: 10.629158492883047 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 37.66330138444901\n",
      "Rows Training set: 982081, Row Validation set: 692693, Rows test set: 33523, Columns: 194, Iteration finished: 17, Time: 49.179240091641745, \n",
      "1048/1048 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7127593596776326\n",
      "interaction_new time: 10.101334198315937 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 37.96451174020767\n",
      "Rows Training set: 987565, Row Validation set: 669620, Rows test set: 31616, Columns: 195, Iteration finished: 18, Time: 48.95405255556106, \n",
      "988/988 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7270493030548095\n",
      "interaction_new time: 9.363538134098054 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 38.934891963005064\n",
      "Rows Training set: 997177, Row Validation set: 642573, Rows test set: 29593, Columns: 194, Iteration finished: 19, Time: 49.19555929899216, \n",
      "925/925 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.75041690270106\n",
      "interaction_new time: 8.95980726480484 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 38.2637841463089\n",
      "Rows Training set: 1011794, Row Validation set: 608305, Rows test set: 27099, Columns: 195, Iteration finished: 20, Time: 48.19723708232244, \n",
      "847/847 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7648090283075969\n",
      "interaction_new time: 8.535103539625803 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 38.50592162211736\n",
      "Rows Training set: 1027397, Row Validation set: 570336, Rows test set: 24154, Columns: 195, Iteration finished: 21, Time: 47.993154593308766, \n",
      "755/755 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7886499881744384\n",
      "interaction_new time: 8.1302618543307 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 39.256895577907564\n",
      "Rows Training set: 1047554, Row Validation set: 526204, Rows test set: 20111, Columns: 192, Iteration finished: 22, Time: 48.355580691496534, \n",
      "629/629 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7967012484868368\n",
      "interaction_new time: 7.946664337317149 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 40.035786934693654\n",
      "Rows Training set: 1070208, Row Validation set: 476179, Rows test set: 17220, Columns: 190, Iteration finished: 23, Time: 48.95470051368078, \n",
      "539/539 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7893673976262411\n",
      "interaction_new time: 7.582066746552785 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 41.58061817089717\n",
      "Rows Training set: 1089247, Row Validation set: 426529, Rows test set: 14187, Columns: 188, Iteration finished: 24, Time: 50.12545310258865, \n",
      "444/444 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7858279466629028\n",
      "interaction_new time: 7.56531761487325 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 41.742507863044736\n",
      "Rows Training set: 1098186, Row Validation set: 380525, Rows test set: 11413, Columns: 185, Iteration finished: 25, Time: 50.25458316405614, \n",
      "357/357 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7819538672765096\n",
      "interaction_new time: 6.546454850832621 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 41.77046357790629\n",
      "Rows Training set: 1101715, Row Validation set: 335610, Rows test set: 8917, Columns: 185, Iteration finished: 26, Time: 49.25384606520335, \n",
      "279/279 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7851193229357402\n",
      "interaction_new time: 6.3586411595344545 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 41.59894959529241\n",
      "Rows Training set: 1095720, Row Validation set: 295844, Rows test set: 7019, Columns: 181, Iteration finished: 27, Time: 48.89264080524445, \n",
      "220/220 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7564216415087383\n",
      "interaction_new time: 5.72910913626353 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 41.78239373366038\n",
      "Rows Training set: 1077092, Row Validation set: 261228, Rows test set: 4798, Columns: 173, Iteration finished: 28, Time: 48.40658597548803, \n",
      "150/150 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7310179630915324\n",
      "interaction_new time: 5.3320506572723385 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 40.12327495018641\n",
      "Rows Training set: 1053884, Row Validation set: 229650, Rows test set: 1597, Columns: 161, Iteration finished: 29, Time: 46.330186545848846, \n",
      "50/50 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Set number of iterations\n",
    "ite = 30 # 30 splits --> iteration = [0;29]\n",
    "\n",
    "# Initialize arrays to store loss and explained variation\n",
    "loss = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "xplained_variation = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "\n",
    "# Initialize arrays to store annual loss and explained variation\n",
    "loss_annual = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "xplained_variation_annual = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "\n",
    "# Initialize arrays to store predicted and actual returns used in portfolio sorts later\n",
    "LR_pred_actual = pd.DataFrame()\n",
    "lasso_pred_actual = pd.DataFrame()\n",
    "NN_pred_actual = pd.DataFrame()\n",
    "\n",
    "for i in range(ite):\n",
    "    \n",
    "    # Compute training, validation, and test set\n",
    "    training, validation, test = DP.complete_data_process(industry_code, returns, FM_data, iteration = i)\n",
    "    \n",
    "    # Split in X and Y\n",
    "    training_x, training_y = pred.XY_split(training)\n",
    "    validation_x, validation_y = pred.XY_split(validation)\n",
    "    test_x, test_y = pred.XY_split(test)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # Algorithm 1: Simple Linear (PCR in Gu, kelly, and Xiu (2020) due to PCA)\n",
    "    # Fit model on training set & predict on test set\n",
    "    LR = lr().fit(training_x, training_y)\n",
    "    LR_pred = LR.predict(test_x)\n",
    "    \n",
    "    # Algoritm 1: Compute loss and explained varation, and combine \n",
    "    # actual and predicted returns. Append all. \n",
    "    LR_loss, LR_explained_var, LR_pred_actual_temp, LR_loss_annual, LR_explained_var_annual = pred.to_append(LR_pred, test_y)\n",
    "    loss.iloc[0, i] = LR_loss\n",
    "    loss_annual.iloc[0, i] = LR_loss_annual\n",
    "    xplained_variation.iloc[0, i] = LR_explained_var\n",
    "    xplained_variation_annual.iloc[0, i] = LR_explained_var_annual\n",
    "    LR_pred_actual = LR_pred_actual.append(LR_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # ALgorithm 2: LASSO\n",
    "    # Fit model on training set and select tuning parameter based on validation set\n",
    "    lambda_grid = pred.lambda_grid(training_x, training_y)\n",
    "    loss_validation = []\n",
    "\n",
    "    for lamb in lambda_grid:\n",
    "        lasso = Lasso(alpha = lamb, tol = 0.001).fit(training_x, training_y)\n",
    "        lasso_pred = lasso.predict(validation_x)\n",
    "        loss_validation.append(pred.loss_function(lasso_pred, validation_y))\n",
    "\n",
    "    # Fit model with error minimizing tuning parameter\n",
    "    lambda_min = lambda_grid[loss_validation.index(min(loss_validation))]\n",
    "    lasso_min = Lasso(alpha = lambda_min).fit(training_x, training_y)\n",
    "    lasso_min_pred = lasso_min.predict(test_x)\n",
    "\n",
    "    # Algorithm 2: Appending \n",
    "    lasso_loss, lasso_explained_var, lasso_pred_actual_temp, lasso_loss_annual, lasso_explained_var_annual = pred.to_append(lasso_min_pred, test_y)\n",
    "    loss.iloc[1, i] = lasso_loss\n",
    "    loss_annual.iloc[1, i] = lasso_loss_annual\n",
    "    xplained_variation.iloc[1, i] = lasso_explained_var\n",
    "    xplained_variation_annual.iloc[1, i] = lasso_explained_var_annual\n",
    "    lasso_pred_actual = lasso_pred_actual.append(lasso_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # ALgorithm 3: NN \n",
    "    \n",
    "    # Build NN architecture (L1 regularization and batch normalization)\n",
    "    model = pred.NN(training_x)\n",
    "\n",
    "    # Define callback for early stopping\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(training_x, training_y, epochs = 50, batch_size = 1000, verbose = 0, validation_data = (validation_x, validation_y), callbacks = [callback])\n",
    "\n",
    "    # Compute predictions\n",
    "    NN_pred = model.predict(test_x)\n",
    "    \n",
    "    # Algorithm 3: Appending\n",
    "    NN_loss, NN_explained_var, NN_pred_actual_temp, NN_loss_annual, NN_explained_var_annual = pred.to_append(NN_pred, test_y)\n",
    "    loss.iloc[2, i] = NN_loss\n",
    "    loss_annual.iloc[2, i] = NN_loss_annual\n",
    "    xplained_variation.iloc[2, i] = NN_explained_var\n",
    "    xplained_variation_annual.iloc[2, i] = NN_explained_var_annual\n",
    "    NN_pred_actual = NN_pred_actual.append(NN_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # Free memory\n",
    "    del training\n",
    "    del validation\n",
    "    del test\n",
    "    del training_x\n",
    "    del training_y\n",
    "    del validation_x\n",
    "    del validation_y\n",
    "    del test_x\n",
    "    del test_y\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b3182e-0172-443c-99ff-07d7ebe8bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "loss.to_csv(os.path.dirname(os.getcwd()) + '\\\\loss.csv', header = True, index = True)\n",
    "xplained_variation.to_csv(os.path.dirname(os.getcwd()) + '\\\\explained_variation.csv', header = True, index = True)\n",
    "loss_annual.to_csv(os.path.dirname(os.getcwd()) + '\\\\loss_annual.csv', header = True, index = True)\n",
    "xplained_variation_annual.to_csv(os.path.dirname(os.getcwd()) + '\\\\explained_variation_annual.csv', header = True, index = True)\n",
    "\n",
    "LR_pred_actual.to_csv(os.path.dirname(os.getcwd()) + '\\\\LR_predictions.csv', header = True, index = False)\n",
    "lasso_pred_actual.to_csv(os.path.dirname(os.getcwd()) + '\\\\lasso_predictions.csv', header = True, index = False)\n",
    "NN_pred_actual.to_csv(os.path.dirname(os.getcwd()) + '\\\\NN_predictions.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9165c412-12a1-4c61-bf20-f2c5c1a075ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "loss = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\loss.csv', index_col = 0)\n",
    "xplained_variation =  pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\explained_variation.csv', index_col = 0)\n",
    "LR_pred_actual = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\LR_predictions.csv')\n",
    "lasso_pred_actual = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\lasso_predictions.csv')\n",
    "NN_pred_actual = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\NN_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30cebda2-23ee-451e-88ca-65880e15695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the single observation in december of 2016\n",
    "LR_pred_actual = LR_pred_actual.iloc[:-1, :]\n",
    "lasso_pred_actual = lasso_pred_actual.iloc[:-1, :]\n",
    "NN_pred_actual = NN_pred_actual.iloc[:-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dbb4a794-fcf2-4e31-ba2f-9ad89c67f99d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Return for each decile at all points in time\n",
    "LR_deciles_ret = pred.portfolio_sorts_1(LR_pred_actual)\n",
    "lasso_deciles_ret = pred.portfolio_sorts_1(lasso_pred_actual)\n",
    "NN_deciles_ret = pred.portfolio_sorts_1(NN_pred_actual)\n",
    "\n",
    "# Return and prediction for 10-1 portfolios at all points in time\n",
    "LR_10_1_ret = pred.decile_10_1(LR_deciles_ret, log = False)\n",
    "lasso_10_1_ret = pred.decile_10_1(lasso_deciles_ret, log = False)\n",
    "NN_10_1_ret = pred.decile_10_1(NN_deciles_ret, log = False)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Log returns of decile portfolios (cannot accumulate returns in pct.)\n",
    "LR_deciles_log_ret = LR_deciles_ret.copy()\n",
    "LR_deciles_log_ret.ret = np.log(1 + LR_deciles_log_ret.ret)\n",
    "LR_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "lasso_deciles_log_ret = lasso_deciles_ret.copy()\n",
    "lasso_deciles_log_ret.ret = np.log(1 + lasso_deciles_log_ret.ret)\n",
    "lasso_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "NN_deciles_log_ret = NN_deciles_ret.copy()\n",
    "NN_deciles_log_ret.ret = np.log(1 + NN_deciles_log_ret.ret)\n",
    "NN_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Log returns of 10-1 portfolio\n",
    "LR_10_1_log_ret = pred.decile_10_1(LR_deciles_log_ret, log = True)\n",
    "lasso_10_1_log_ret = pred.decile_10_1(lasso_deciles_log_ret, log = True)\n",
    "NN_10_1_log_ret = pred.decile_10_1(NN_deciles_log_ret, log = True)\n",
    "\n",
    "'''\n",
    "# Old incorrect way of computing log returns of 10-1 portfolio\n",
    "# I mistakenly subtract the returns and then take the log\n",
    "# I ought to take the log and then subtract \n",
    "# Log returns of 10-1 portfolio\n",
    "LR_10_1_log_ret = LR_10_1_ret.copy()\n",
    "LR_10_1_log_ret.ret = np.log(1 + LR_10_1_log_ret.ret)\n",
    "LR_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "lasso_10_1_log_ret = lasso_10_1_ret.copy()\n",
    "lasso_10_1_log_ret.ret = np.log(1 + lasso_10_1_log_ret.ret)\n",
    "lasso_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "NN_10_1_log_ret = NN_10_1_ret.copy()\n",
    "NN_10_1_log_ret.ret = np.log(1 + NN_10_1_log_ret.ret)\n",
    "NN_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "'''\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Accumualtive log returns and predicted returns \n",
    "# of all deciles (cannot accumulate returns in pct.)\n",
    "LR_cum_log_ret = pred.portfolio_sorts_acc_return(LR_deciles_log_ret)\n",
    "lasso_cum_log_ret = pred.portfolio_sorts_acc_return(lasso_deciles_log_ret)\n",
    "NN_cum_log_ret = pred.portfolio_sorts_acc_return(NN_deciles_log_ret)\n",
    "\n",
    "# Monthly average return and std. deviation, and annualized SR\n",
    "# of both predicted and actual returns\n",
    "LR_mean, LR_std, LR_sr = pred.portfolio_sorts_SR(LR_deciles_ret)\n",
    "lasso_mean, lasso_std, lasso_sr = pred.portfolio_sorts_SR(lasso_deciles_ret)\n",
    "NN_mean, NN_std, NN_sr = pred.portfolio_sorts_SR(NN_deciles_ret)\n",
    "\n",
    "# Monthly average return and std. deviation, and annualized SR \n",
    "# for 10-1 portfolio\n",
    "LR_10_1_mean, LR_10_1_sd, LR_10_1_sr = pred.portfolio_sorts_SR(LR_10_1_ret)\n",
    "lasso_10_1_mean, lasso_10_1_sd, lasso_10_1_sr = pred.portfolio_sorts_SR(lasso_10_1_ret)\n",
    "NN_10_1_mean, NN_10_1_sd, NN_10_1_sr = pred.portfolio_sorts_SR(NN_10_1_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a2165937-8930-4094-b3c3-d3d4c2d02f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of all deciles for each ML model\n",
    "pred.cumulative_ret_fig(data = LR_deciles_log_ret, name = \"LR_cumulative_log_ret\", market = SP500, save_fig = True, hide = True)\n",
    "pred.cumulative_ret_fig(data = lasso_deciles_log_ret, name = \"lasso_cumulative_log_ret\", market = SP500, save_fig = True, hide = True)\n",
    "pred.cumulative_ret_fig(data = NN_deciles_log_ret, name = \"NN_cumulative_log_ret\", market = SP500, save_fig = True, hide = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b1fa70c0-0912-4900-aa9b-a92889f5092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of 10-1 portfolio for each ML model\n",
    "pred.portfolio_10_1_fig(name = \"10-1_portfolio_cumulative_log_ret\", market = SP500, save_fig = True, hide = True, data1 = LR_10_1_log_ret, data2 = lasso_10_1_log_ret, data3 = NN_10_1_log_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "adc7fff4-1c51-49db-b767-eae67bb7437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of 1st and 10th decile of specified ML models \n",
    "pred.deciles_10_1_fig(name = \"deciles_10_1_cumulative_log_ret\", market = SP500, save_fig = True, hide = True, data1 = LR_deciles_log_ret, data2 = lasso_deciles_log_ret, data3 = NN_deciles_log_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3ea367d1-d611-4efc-b97e-e854d14dcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for export to R so as to customize for tables \n",
    "\n",
    "# Table 1: monthly loss and explained variation (Multiply with 100 to get pct.)\n",
    "monthly_pricing_error = loss.mean(axis = 1) * 100\n",
    "monthly_xplained_variation = xplained_variation.mean(axis = 1) * 100\n",
    "table1 = pd.concat([monthly_pricing_error, monthly_xplained_variation], axis = 1)\n",
    "table1.columns = [\"Squared Pricing Error\", \"Explained Variation\"]\n",
    "table1 = table1.transpose()\n",
    "table1.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table1.to_csv(os.path.dirname(os.getcwd()) + '\\\\table1_data.csv', header = True, index = True)\n",
    "\n",
    "''' Incorrect as is. Consult Predictionstep1.py, to_append function\n",
    "# Table 2: Annual loss and explained variation (Multiply with 100 to get pct.)\n",
    "annual_pricing_error = loss_annual.mean(axis = 1) * 100\n",
    "annual_xplained_variation = xplained_variation_annual.mean(axis = 1) * 100\n",
    "table2 = pd.concat([annual_pricing_error, annual_xplained_variation], axis = 1)\n",
    "table2.columns = [\"Squared Pricing Error\", \"Explained Variation\"]\n",
    "table2 = table2.transpose()\n",
    "table2.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table2.to_csv(os.path.dirname(os.getcwd()) + '\\\\table2_data.csv', header = True, index = True)\n",
    "'''\n",
    "\n",
    "# Table 3: (Multiply with 100 to get pct.)\n",
    "table3_LR = pd.concat([LR_mean * 100, LR_std.ret * 100, LR_sr.ret], axis = 1).round(2)\n",
    "table3_LR.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_lasso = pd.concat([lasso_mean * 100 , lasso_std.ret * 100, lasso_sr.ret], axis = 1).round(2)\n",
    "table3_lasso.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_NN = pd.concat([NN_mean * 100, NN_std.ret * 100, NN_sr.ret], axis = 1).round(2)\n",
    "table3_NN.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "\n",
    "# Table 3: Add 10-1 portfolio row for each ML model\n",
    "LR_portfolio_10_1_row = pd.concat((LR_10_1_mean * 100, LR_10_1_sd.ret * 100, LR_10_1_sr.ret), axis = 1)\n",
    "LR_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_LR = pd.concat((table3_LR, LR_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "lasso_portfolio_10_1_row = pd.concat((lasso_10_1_mean * 100, lasso_10_1_sd.ret * 100, lasso_10_1_sr.ret), axis = 1)\n",
    "lasso_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_lasso = pd.concat((table3_lasso, lasso_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "NN_portfolio_10_1_row = pd.concat((NN_10_1_mean * 100, NN_10_1_sd.ret * 100, NN_10_1_sr.ret), axis = 1)\n",
    "NN_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_NN = pd.concat((table3_NN, NN_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "# Table 4: Tabel for appendix figurerne (skal bruge LR_cum, lasso_cum, NN_cum) og så bare kun ret søjlen. Har så tabel der viser end point for figurene (denne tabel kommer i appendix) \n",
    "table4 = pd.concat([LR_cum_log_ret.log_ret, lasso_cum_log_ret.log_ret, NN_cum_log_ret.log_ret], axis = 1).round(3)\n",
    "table4.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table4 = table4.transpose()\n",
    "table4.to_csv(os.path.dirname(os.getcwd()) + '\\\\table4_data.csv', header = True, index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ec975aca-900c-4d04-98ac-f97aa08e01c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data for CAPM, FF3, and FF5 \n",
    "FF5 = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\F-F_Research_Data_5_Factors_2x3.csv')\n",
    "FF5.rename(columns = {'Unnamed: 0':'date'}, inplace = True)\n",
    "FF5 = FF5[(FF5[\"date\"] >= 198701) & (FF5[\"date\"] < 201612)]\n",
    "FF5 = FF5.set_index('date')\n",
    "FF5 = FF5[[\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]]\n",
    "\n",
    "# Define FF3 and CAPM from FF5\n",
    "FF3 = FF5[[\"Mkt-RF\", \"SMB\", \"HML\"]]\n",
    "CAPM = FF5[\"Mkt-RF\"]\n",
    "\n",
    "# LR regressions\n",
    "CAPM_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# Lasso regressions\n",
    "CAPM_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# NN regressions\n",
    "CAPM_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# CAPM table\n",
    "CAPM_params, CAPM_tvalues = pred.LM_tables(CAPM_LR, CAPM_lasso, CAPM_NN)\n",
    "CAPM_params.index = [\"Constant\", \"Mkt-RF\"]\n",
    "CAPM_tvalues.index = [\"Constant\", \"Mkt-RF\"]\n",
    "CAPM_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\CAPM_params.csv', header = True, index = True) \n",
    "CAPM_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\CAPM_tvalues.csv', header = True, index = True) \n",
    "\n",
    "# FF3 table\n",
    "FF3_params, FF3_tvalues = pred.LM_tables(FF3_LR, FF3_lasso, FF3_NN)\n",
    "FF3_params.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\"]\n",
    "FF3_tvalues.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\"]\n",
    "FF3_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF3_params.csv', header = True, index = True) \n",
    "FF3_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF3_tvalues.csv', header = True, index = True) \n",
    "\n",
    "# FF5 table \n",
    "FF5_params, FF5_tvalues = pred.LM_tables(FF5_LR, FF5_lasso, FF5_NN)\n",
    "FF5_params.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "FF5_tvalues.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "FF5_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF5_params.csv', header = True, index = True) \n",
    "FF5_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF5_tvalues.csv', header = True, index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bbef2e10-33fd-4105-a960-6c5932978af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(FF3_LR.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78618e90-443a-4ca2-9a69-914ee0e2f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skal jeg lave en normals OLS på ikke PCA data? bare som totalt benchmark? -- køre det på FM_data + industry codes så -- ingen interaktion terms eller PCA "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
