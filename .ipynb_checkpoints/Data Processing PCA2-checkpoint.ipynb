{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de3de21-6556-4f1e-80e2-80a89e701949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load dependencies\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import itertools as it\n",
    "import DataProcessFunctions as DP\n",
    "import PredictionStep1 as pred\n",
    "import SupportFunctions as supp\n",
    "import LinearModelEstimation as lm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import statsmodels.api as sm  \n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import callbacks\n",
    "from matplotlib.pyplot import cm\n",
    "import time as time \n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Disable scientific notation (e) in numpy\n",
    "# Disable futurewarnings\n",
    "np.set_printoptions(suppress=True)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8f707-f8b3-4490-85d4-9463d4525581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize warning log container \n",
    "log = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76be5616-1015-4f3d-b1bc-de5deed9e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before downcast: 1.769 GB and float64    102\n",
      "int64        3\n",
      "dtype: int64\n",
      "After downcast: 0.878 GB and float32    102\n",
      "int32        2\n",
      "int8         1\n",
      "dtype: int64\n",
      "Before downcast: 0.026 GB and float64    1\n",
      "dtype: int64\n",
      "After downcast: 0.018 GB and float32    1\n",
      "dtype: int64\n",
      "Before downcast: 0.026 GB and float64    1\n",
      "dtype: int64\n",
      "After downcast: 0.018 GB and float32    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load Pre-processed data from Data Processing PCA.ipynb\n",
    "FM_data = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\FM2_data.csv')\n",
    "returns = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\returns2_data.csv').set_index([\"permno\", \"date\"])\n",
    "industry_code = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\industry2_codes.csv').set_index([\"permno\", \"date\"])\n",
    "\n",
    "# Load excess SP500 log returns data \n",
    "SP500 = pd.read_excel(os.path.dirname(os.getcwd()) + '\\\\SP500.xlsx')[\"Cum return\"]\n",
    "\n",
    "supp.downcast(FM_data)\n",
    "supp.downcast(returns)\n",
    "supp.downcast(industry_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa87af2c-5685-4f1c-a59f-1cbcb4e35d77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data_processing_new time: 0.6868337710698446\n",
      "interaction_new time: 8.627713119983673 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 38.513426605860396\n",
      "Rows Training set: 1011794, Row Validation set: 608305, Rows test set: 27099, Columns: 195, Iteration finished: 20, Time: 47.984201987584434, \n",
      "847/847 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7171700199445089\n",
      "interaction_new time: 8.681127909819285 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 38.48274031082789\n",
      "Rows Training set: 1027397, Row Validation set: 570336, Rows test set: 24154, Columns: 195, Iteration finished: 21, Time: 48.04004948933919, \n",
      "755/755 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7681570291519165\n",
      "interaction_new time: 8.252127663294475 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 40.79441001812617\n",
      "Rows Training set: 1047554, Row Validation set: 526204, Rows test set: 20111, Columns: 192, Iteration finished: 22, Time: 50.046441260973616, \n",
      "629/629 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7847103476524353\n",
      "interaction_new time: 7.897066613038381 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 40.949536911646526\n",
      "Rows Training set: 1070208, Row Validation set: 476179, Rows test set: 17220, Columns: 190, Iteration finished: 23, Time: 49.80568700234095, \n",
      "539/539 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7937341729799906\n",
      "interaction_new time: 7.3541359027226765 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 41.36046581665675\n",
      "Rows Training set: 1089247, Row Validation set: 426529, Rows test set: 14187, Columns: 188, Iteration finished: 24, Time: 49.6801694393158, \n",
      "444/444 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.8086010257403056\n",
      "interaction_new time: 6.621172106266021 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 41.21681189139684\n",
      "Rows Training set: 1098186, Row Validation set: 380525, Rows test set: 11413, Columns: 185, Iteration finished: 25, Time: 48.80826689402262, \n",
      "357/357 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7832498709360759\n",
      "interaction_new time: 6.8657629013061525 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 43.17518974542618\n",
      "Rows Training set: 1101715, Row Validation set: 335610, Rows test set: 8917, Columns: 185, Iteration finished: 26, Time: 50.98341998259227, \n",
      "279/279 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7877025326093038\n",
      "interaction_new time: 6.198922149340311 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 42.946260221799214\n",
      "Rows Training set: 1095720, Row Validation set: 295844, Rows test set: 7019, Columns: 181, Iteration finished: 27, Time: 50.13962065776189, \n",
      "220/220 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7798530141512553\n",
      "interaction_new time: 5.593898403644562 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 42.152706619103746\n",
      "Rows Training set: 1077092, Row Validation set: 261228, Rows test set: 4798, Columns: 173, Iteration finished: 28, Time: 48.66662139495214, \n",
      "150/150 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.740647840499878\n",
      "interaction_new time: 5.170649667580922 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 39.14104372660319\n",
      "Rows Training set: 1053884, Row Validation set: 229650, Rows test set: 1597, Columns: 161, Iteration finished: 29, Time: 45.18568609158198, \n",
      "50/50 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Set number of iterations\n",
    "ite = 30 # 30 splits --> iteration = [0;29]\n",
    "\n",
    "# Initialize arrays to store loss and explained variation\n",
    "loss = pd.DataFrame(columns = range(ite, 30), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "xplained_variation = pd.DataFrame(columns = range(ite, 30), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "\n",
    "# Initialize arrays to store annual loss and explained variation\n",
    "loss_annual = pd.DataFrame(columns = range(ite, 30), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "xplained_variation_annual = pd.DataFrame(columns = range(ite, 30), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "\n",
    "# Initialize arrays to store predicted and actual returns used in portfolio sorts later\n",
    "LR_pred_actual = pd.DataFrame()\n",
    "lasso_pred_actual = pd.DataFrame()\n",
    "NN_pred_actual = pd.DataFrame()\n",
    "\n",
    "for i in range(ite):\n",
    "    \n",
    "    # Compute training, validation, and test set\n",
    "    training, validation, test = DP.complete_data_process(industry_code, returns, FM_data, iteration = i)\n",
    "    \n",
    "    # Split in X and Y\n",
    "    training_x, training_y = pred.XY_split(training)\n",
    "    validation_x, validation_y = pred.XY_split(validation)\n",
    "    test_x, test_y = pred.XY_split(test)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # Algorithm 1: Simple Linear (PCR in Gu, kelly, and Xiu (2020) due to PCA)\n",
    "    # Fit model on training set & predict on test set\n",
    "    LR = lr().fit(training_x, training_y)\n",
    "    LR_pred = LR.predict(test_x)\n",
    "    \n",
    "    # Algoritm 1: Compute loss and explained varation, and combine \n",
    "    # actual and predicted returns. Append all. \n",
    "    LR_loss, LR_explained_var, LR_pred_actual_temp, LR_loss_annual, LR_explained_var_annual = pred.to_append(LR_pred, test_y)\n",
    "    loss.iloc[0, i] = LR_loss\n",
    "    loss_annual.iloc[0, i] = LR_loss_annual\n",
    "    xplained_variation.iloc[0, i] = LR_explained_var\n",
    "    xplained_variation_annual.iloc[0, i] = LR_explained_var_annual\n",
    "    LR_pred_actual = LR_pred_actual.append(LR_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # ALgorithm 2: LASSO\n",
    "    # Fit model on training set and select tuning parameter based on validation set\n",
    "    lambda_grid = pred.lambda_grid(training_x, training_y)\n",
    "    loss_validation = []\n",
    "\n",
    "    for lamb in lambda_grid:\n",
    "        lasso = Lasso(alpha = lamb, tol = 0.001).fit(training_x, training_y)\n",
    "        lasso_pred = lasso.predict(validation_x)\n",
    "        loss_validation.append(pred.loss_function(lasso_pred, validation_y))\n",
    "\n",
    "    # Fit model with error minimizing tuning parameter\n",
    "    lambda_min = lambda_grid[loss_validation.index(min(loss_validation))]\n",
    "    lasso_min = Lasso(alpha = lambda_min).fit(training_x, training_y)\n",
    "    lasso_min_pred = lasso_min.predict(test_x)\n",
    "\n",
    "    # Algorithm 2: Appending \n",
    "    lasso_loss, lasso_explained_var, lasso_pred_actual_temp, lasso_loss_annual, lasso_explained_var_annual = pred.to_append(lasso_min_pred, test_y)\n",
    "    loss.iloc[1, i] = lasso_loss\n",
    "    loss_annual.iloc[1, i] = lasso_loss_annual\n",
    "    xplained_variation.iloc[1, i] = lasso_explained_var\n",
    "    xplained_variation_annual.iloc[1, i] = lasso_explained_var_annual\n",
    "    lasso_pred_actual = lasso_pred_actual.append(lasso_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # ALgorithm 3: NN \n",
    "    # Build NN architecture (L1 regularization and batch normalization)\n",
    "    model = pred.NN(training_x)\n",
    "\n",
    "    # Define callback for early stopping\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(training_x, training_y, epochs = 50, batch_size = 500, verbose = 0, validation_data = (validation_x, validation_y), callbacks = [callback])\n",
    "\n",
    "    # Compute predictions\n",
    "    NN_pred = model.predict(test_x)\n",
    "    \n",
    "    # Algorithm 3: Appending\n",
    "    NN_loss, NN_explained_var, NN_pred_actual_temp, NN_loss_annual, NN_explained_var_annual = pred.to_append(NN_pred, test_y)\n",
    "    loss.iloc[2, i] = NN_loss\n",
    "    loss_annual.iloc[2, i] = NN_loss_annual\n",
    "    xplained_variation.iloc[2, i] = NN_explained_var\n",
    "    xplained_variation_annual.iloc[2, i] = NN_explained_var_annual\n",
    "    NN_pred_actual = NN_pred_actual.append(NN_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # Free memory\n",
    "    del training\n",
    "    del validation\n",
    "    del test\n",
    "    del training_x\n",
    "    del training_y\n",
    "    del validation_x\n",
    "    del validation_y\n",
    "    del test_x\n",
    "    del test_y\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60b3182e-0172-443c-99ff-07d7ebe8bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.to_csv(os.path.dirname(os.getcwd()) + '\\\\loss_2.csv', header = True, index = True)\n",
    "xplained_variation.to_csv(os.path.dirname(os.getcwd()) + '\\\\explained_variation_2.csv', header = True, index = True)\n",
    "loss_annual.to_csv(os.path.dirname(os.getcwd()) + '\\\\loss_annual_2.csv', header = True, index = True)\n",
    "xplained_variation_annual.to_csv(os.path.dirname(os.getcwd()) + '\\\\explained_variation_annual_2.csv', header = True, index = True)\n",
    "\n",
    "LR_pred_actual.to_csv(os.path.dirname(os.getcwd()) + '\\\\LR_predictions_2.csv', header = True, index = False)\n",
    "lasso_pred_actual.to_csv(os.path.dirname(os.getcwd()) + '\\\\lasso_predictions_2.csv', header = True, index = False)\n",
    "NN_pred_actual.to_csv(os.path.dirname(os.getcwd()) + '\\\\NN_predictions_2.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9165c412-12a1-4c61-bf20-f2c5c1a075ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_first = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\loss.csv', index_col = 0)\n",
    "xplained_variation_first =  pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\explained_variation.csv', index_col = 0)\n",
    "LR_pred_actual_first = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\LR_predictions.csv')\n",
    "lasso_pred_actual_first = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\lasso_predictions.csv')\n",
    "NN_pred_actual_first = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\NN_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b6e5524-5b6c-4929-9456-6e6a14d82c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = pd.concat((loss_first.iloc[:, :20], loss), axis = 1)\n",
    "xplained_variation = pd.concat((xplained_variation_first.iloc[:, :20], xplained_variation), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f64daeba-da51-4d79-8804-a097dd5013c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_pred_actual = pd.concat((LR_pred_actual_first, LR_pred_actual), axis = 0)\n",
    "lasso_pred_actual = pd.concat((lasso_pred_actual_first, lasso_pred_actual), axis = 0)\n",
    "NN_pred_actual = pd.concat((NN_pred_actual_first, NN_pred_actual), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f337a4ce-5ebf-4e64-80ca-5442c8635aa1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bin edges must be unique: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]).\nYou can drop duplicate edges by setting the 'duplicates' kwarg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m LR_deciles_ret \u001b[38;5;241m=\u001b[39m \u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mportfolio_sorts_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLR_pred_actual\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - Københavns Universitet\\Documents\\Økonomi - Kandidat\\6. Semester\\Speciale\\Masters-Thesis\\PredictionStep1.py:254\u001b[0m, in \u001b[0;36mportfolio_sorts_1\u001b[1;34m(pred_actual)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(pred_actual\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m pred_actual\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# Sort predicted returns at each point in time into deciles\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m pred_actual[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecile\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpred_actual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqcut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# Compute average return for each decile at all points in time\u001b[39;00m\n\u001b[0;32m    257\u001b[0m deciles \u001b[38;5;241m=\u001b[39m pred_actual\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecile\u001b[39m\u001b[38;5;124m\"\u001b[39m])[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mret\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:428\u001b[0m, in \u001b[0;36mSeriesGroupBy.transform\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(klass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    426\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_transform_template)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[0;32m    429\u001b[0m         func, \u001b[38;5;241m*\u001b[39margs, engine\u001b[38;5;241m=\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    430\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1642\u001b[0m, in \u001b[0;36mGroupBy._transform\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1639\u001b[0m func \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mget_cython_func(func) \u001b[38;5;129;01mor\u001b[39;00m func\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m func \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m base\u001b[38;5;241m.\u001b[39mtransform_kernel_allowlist:\n\u001b[0;32m   1645\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a valid function name for transform(name)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:459\u001b[0m, in \u001b[0;36mSeriesGroupBy._transform_general\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;66;03m# this setattr is needed for test_transform_lambda_with_datetimetz\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(group, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[1;32m--> 459\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(group, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    461\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(klass(res, index\u001b[38;5;241m=\u001b[39mgroup\u001b[38;5;241m.\u001b[39mindex))\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# check for empty \"results\" to avoid concat ValueError\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Københavns Universitet\\Documents\\Økonomi - Kandidat\\6. Semester\\Speciale\\Masters-Thesis\\PredictionStep1.py:254\u001b[0m, in \u001b[0;36mportfolio_sorts_1.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(pred_actual\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m pred_actual\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# Sort predicted returns at each point in time into deciles\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m pred_actual[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecile\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pred_actual\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqcut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# Compute average return for each decile at all points in time\u001b[39;00m\n\u001b[0;32m    257\u001b[0m deciles \u001b[38;5;241m=\u001b[39m pred_actual\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecile\u001b[39m\u001b[38;5;124m\"\u001b[39m])[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mret\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\reshape\\tile.py:378\u001b[0m, in \u001b[0;36mqcut\u001b[1;34m(x, q, labels, retbins, precision, duplicates)\u001b[0m\n\u001b[0;32m    375\u001b[0m x_np \u001b[38;5;241m=\u001b[39m x_np[\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misnan(x_np)]\n\u001b[0;32m    376\u001b[0m bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mquantile(x_np, quantiles)\n\u001b[1;32m--> 378\u001b[0m fac, bins \u001b[38;5;241m=\u001b[39m \u001b[43m_bins_to_cuts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_lowest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mduplicates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduplicates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _postprocess_for_cut(fac, bins, retbins, dtype, original)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\reshape\\tile.py:419\u001b[0m, in \u001b[0;36m_bins_to_cuts\u001b[1;34m(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_bins) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m duplicates \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 419\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    420\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBin edges must be unique: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(bins)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    421\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can drop duplicate edges by setting the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduplicates\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m kwarg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    422\u001b[0m         )\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    424\u001b[0m         bins \u001b[38;5;241m=\u001b[39m unique_bins\n",
      "\u001b[1;31mValueError\u001b[0m: Bin edges must be unique: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]).\nYou can drop duplicate edges by setting the 'duplicates' kwarg"
     ]
    }
   ],
   "source": [
    "LR_deciles_ret = pred.portfolio_sorts_1(LR_pred_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbb4a794-fcf2-4e31-ba2f-9ad89c67f99d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bin edges must be unique: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]).\nYou can drop duplicate edges by setting the 'duplicates' kwarg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Return for each decile at all points in time\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m LR_deciles_ret \u001b[38;5;241m=\u001b[39m \u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mportfolio_sorts_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLR_pred_actual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m lasso_deciles_ret \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mportfolio_sorts_1(lasso_pred_actual)\n\u001b[0;32m      4\u001b[0m NN_deciles_ret \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mportfolio_sorts_1(NN_pred_actual)\n",
      "File \u001b[1;32m~\\OneDrive - Københavns Universitet\\Documents\\Økonomi - Kandidat\\6. Semester\\Speciale\\Masters-Thesis\\PredictionStep1.py:254\u001b[0m, in \u001b[0;36mportfolio_sorts_1\u001b[1;34m(pred_actual)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(pred_actual\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m pred_actual\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# Sort predicted returns at each point in time into deciles\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m pred_actual[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecile\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpred_actual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqcut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# Compute average return for each decile at all points in time\u001b[39;00m\n\u001b[0;32m    257\u001b[0m deciles \u001b[38;5;241m=\u001b[39m pred_actual\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecile\u001b[39m\u001b[38;5;124m\"\u001b[39m])[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mret\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:428\u001b[0m, in \u001b[0;36mSeriesGroupBy.transform\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(klass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    426\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_transform_template)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[0;32m    429\u001b[0m         func, \u001b[38;5;241m*\u001b[39margs, engine\u001b[38;5;241m=\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    430\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1642\u001b[0m, in \u001b[0;36mGroupBy._transform\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1639\u001b[0m func \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mget_cython_func(func) \u001b[38;5;129;01mor\u001b[39;00m func\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m func \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m base\u001b[38;5;241m.\u001b[39mtransform_kernel_allowlist:\n\u001b[0;32m   1645\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a valid function name for transform(name)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:459\u001b[0m, in \u001b[0;36mSeriesGroupBy._transform_general\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;66;03m# this setattr is needed for test_transform_lambda_with_datetimetz\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(group, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[1;32m--> 459\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(group, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    461\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(klass(res, index\u001b[38;5;241m=\u001b[39mgroup\u001b[38;5;241m.\u001b[39mindex))\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# check for empty \"results\" to avoid concat ValueError\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Københavns Universitet\\Documents\\Økonomi - Kandidat\\6. Semester\\Speciale\\Masters-Thesis\\PredictionStep1.py:254\u001b[0m, in \u001b[0;36mportfolio_sorts_1.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(pred_actual\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m pred_actual\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# Sort predicted returns at each point in time into deciles\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m pred_actual[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecile\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pred_actual\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqcut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# Compute average return for each decile at all points in time\u001b[39;00m\n\u001b[0;32m    257\u001b[0m deciles \u001b[38;5;241m=\u001b[39m pred_actual\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecile\u001b[39m\u001b[38;5;124m\"\u001b[39m])[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mret\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\reshape\\tile.py:378\u001b[0m, in \u001b[0;36mqcut\u001b[1;34m(x, q, labels, retbins, precision, duplicates)\u001b[0m\n\u001b[0;32m    375\u001b[0m x_np \u001b[38;5;241m=\u001b[39m x_np[\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misnan(x_np)]\n\u001b[0;32m    376\u001b[0m bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mquantile(x_np, quantiles)\n\u001b[1;32m--> 378\u001b[0m fac, bins \u001b[38;5;241m=\u001b[39m \u001b[43m_bins_to_cuts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_lowest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mduplicates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduplicates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _postprocess_for_cut(fac, bins, retbins, dtype, original)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\reshape\\tile.py:419\u001b[0m, in \u001b[0;36m_bins_to_cuts\u001b[1;34m(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_bins) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m duplicates \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 419\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    420\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBin edges must be unique: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(bins)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    421\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can drop duplicate edges by setting the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduplicates\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m kwarg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    422\u001b[0m         )\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    424\u001b[0m         bins \u001b[38;5;241m=\u001b[39m unique_bins\n",
      "\u001b[1;31mValueError\u001b[0m: Bin edges must be unique: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]).\nYou can drop duplicate edges by setting the 'duplicates' kwarg"
     ]
    }
   ],
   "source": [
    "# Return for each decile at all points in time\n",
    "LR_deciles_ret = pred.portfolio_sorts_1(LR_pred_actual)\n",
    "lasso_deciles_ret = pred.portfolio_sorts_1(lasso_pred_actual)\n",
    "NN_deciles_ret = pred.portfolio_sorts_1(NN_pred_actual)\n",
    "\n",
    "# Return and prediction for 10-1 portfolios at all points in time\n",
    "LR_10_1_ret = pred.decile_10_1(LR_deciles_ret)\n",
    "lasso_10_1_ret = pred.decile_10_1(lasso_deciles_ret)\n",
    "NN_10_1_ret = pred.decile_10_1(NN_deciles_ret)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Log returns of decile portfolios (cannot accumulate returns in pct.)\n",
    "LR_deciles_log_ret = LR_deciles_ret.copy()\n",
    "LR_deciles_log_ret.ret = np.log(1 + LR_deciles_log_ret.ret)\n",
    "LR_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "lasso_deciles_log_ret = lasso_deciles_ret.copy()\n",
    "lasso_deciles_log_ret.ret = np.log(1 + lasso_deciles_log_ret.ret)\n",
    "lasso_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "NN_deciles_log_ret = NN_deciles_ret.copy()\n",
    "NN_deciles_log_ret.ret = np.log(1 + NN_deciles_log_ret.ret)\n",
    "NN_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Log returns of 10-1 portfolio\n",
    "LR_10_1_log_ret = LR_10_1_ret.copy()\n",
    "LR_10_1_log_ret.ret = np.log(1 + LR_10_1_log_ret.ret)\n",
    "LR_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "lasso_10_1_log_ret = lasso_10_1_ret.copy()\n",
    "lasso_10_1_log_ret.ret = np.log(1 + lasso_10_1_log_ret.ret)\n",
    "lasso_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "NN_10_1_log_ret = NN_10_1_ret.copy()\n",
    "NN_10_1_log_ret.ret = np.log(1 + NN_10_1_log_ret.ret)\n",
    "NN_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Accumualtive log returns and predicted returns \n",
    "# of all deciles (cannot accumulate returns in pct.)\n",
    "LR_cum_log_ret = pred.portfolio_sorts_acc_return(LR_deciles_log_ret)\n",
    "lasso_cum_log_ret = pred.portfolio_sorts_acc_return(lasso_deciles_log_ret)\n",
    "NN_cum_log_ret = pred.portfolio_sorts_acc_return(NN_deciles_log_ret)\n",
    "\n",
    "# Monthly average return and std. deviation, and annualized SR\n",
    "# of both predicted and actual returns\n",
    "LR_mean, LR_std, LR_sr = pred.portfolio_sorts_SR(LR_deciles_ret)\n",
    "lasso_mean, lasso_std, lasso_sr = pred.portfolio_sorts_SR(lasso_deciles_ret)\n",
    "NN_mean, NN_std, NN_sr = pred.portfolio_sorts_SR(NN_deciles_ret)\n",
    "\n",
    "# Monthly average return and std. deviation, and annualized SR \n",
    "# for 10-1 portfolio\n",
    "LR_10_1_mean, LR_10_1_sd, LR_10_1_sr = pred.portfolio_sorts_SR(LR_10_1_ret)\n",
    "lasso_10_1_mean, lasso_10_1_sd, lasso_10_1_sr = pred.portfolio_sorts_SR(lasso_10_1_ret)\n",
    "NN_10_1_mean, NN_10_1_sd, NN_10_1_sr = pred.portfolio_sorts_SR(NN_10_1_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "id": "a2165937-8930-4094-b3c3-d3d4c2d02f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of all deciles for each ML model\n",
    "pred.cumulative_ret_fig(data = LR_deciles_log_ret, name = \"LR_cumulative_log_ret\", market = SP500, save_fig = True, hide = True)\n",
    "pred.cumulative_ret_fig(data = lasso_deciles_log_ret, name = \"lasso_cumulative_log_ret\", market = SP500, save_fig = True, hide = True)\n",
    "pred.cumulative_ret_fig(data = NN_deciles_log_ret, name = \"NN_cumulative_log_ret\", market = SP500, save_fig = True, hide = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "b1fa70c0-0912-4900-aa9b-a92889f5092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of 10-1 portfolio for each ML model\n",
    "pred.portfolio_10_1_fig(name = \"10-1_portfolio_cumulative_log_ret\", market = SP500, save_fig = True, hide = True, data1 = LR_10_1_log_ret, data2 = lasso_10_1_log_ret, data3 = NN_10_1_log_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "id": "adc7fff4-1c51-49db-b767-eae67bb7437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of 1st and 10th decile of specified ML models \n",
    "pred.deciles_10_1_fig(name = \"deciles_10_1_cumulative_log_ret\", market = SP500, save_fig = True, hide = True, data1 = LR_deciles_log_ret, data2 = lasso_deciles_log_ret, data3 = NN_deciles_log_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "3ea367d1-d611-4efc-b97e-e854d14dcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for export to R so as to customize for tables \n",
    "\n",
    "# Table 1: monthly loss and explained variation (Multiply with 100 to get pct.)\n",
    "monthly_pricing_error = loss.mean(axis = 1) * 100\n",
    "monthly_xplained_variation = xplained_variation.mean(axis = 1) * 100\n",
    "table1 = pd.concat([monthly_pricing_error, monthly_xplained_variation], axis = 1)\n",
    "table1.columns = [\"Squared Pricing Error\", \"Explained Variation\"]\n",
    "table1 = table1.transpose()\n",
    "table1.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table1.to_csv(os.path.dirname(os.getcwd()) + '\\\\table1_data.csv', header = True, index = True)\n",
    "\n",
    "''' Incorrect as is. Consult Predictionstep1.py, to_append function\n",
    "# Table 2: Annual loss and explained variation (Multiply with 100 to get pct.)\n",
    "annual_pricing_error = loss_annual.mean(axis = 1) * 100\n",
    "annual_xplained_variation = xplained_variation_annual.mean(axis = 1) * 100\n",
    "table2 = pd.concat([annual_pricing_error, annual_xplained_variation], axis = 1)\n",
    "table2.columns = [\"Squared Pricing Error\", \"Explained Variation\"]\n",
    "table2 = table2.transpose()\n",
    "table2.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table2.to_csv(os.path.dirname(os.getcwd()) + '\\\\table2_data.csv', header = True, index = True)\n",
    "'''\n",
    "\n",
    "# Table 3: (Multiply with 100 to get pct.)\n",
    "table3_LR = pd.concat([LR_mean * 100, LR_std.ret * 100, LR_sr.ret], axis = 1).round(2)\n",
    "table3_LR.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_lasso = pd.concat([lasso_mean * 100 , lasso_std.ret * 100, lasso_sr.ret], axis = 1).round(2)\n",
    "table3_lasso.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_NN = pd.concat([NN_mean * 100, NN_std.ret * 100, NN_sr.ret], axis = 1).round(2)\n",
    "table3_NN.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "\n",
    "# Table 3: Add 10-1 portfolio row for each ML model\n",
    "LR_portfolio_10_1_row = pd.concat((LR_10_1_mean * 100, LR_10_1_sd.ret * 100, LR_10_1_sr.ret), axis = 1)\n",
    "LR_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_LR = pd.concat((table3_LR, LR_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "lasso_portfolio_10_1_row = pd.concat((lasso_10_1_mean * 100, lasso_10_1_sd.ret * 100, lasso_10_1_sr.ret), axis = 1)\n",
    "lasso_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_lasso = pd.concat((table3_lasso, lasso_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "NN_portfolio_10_1_row = pd.concat((NN_10_1_mean * 100, NN_10_1_sd.ret * 100, NN_10_1_sr.ret), axis = 1)\n",
    "NN_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_NN = pd.concat((table3_NN, NN_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "# Table 4: Tabel for appendix figurerne (skal bruge LR_cum, lasso_cum, NN_cum) og så bare kun ret søjlen. Har så tabel der viser end point for figurene (denne tabel kommer i appendix) \n",
    "table4 = pd.concat([LR_cum_log_ret.log_ret, lasso_cum_log_ret.log_ret, NN_cum_log_ret.log_ret], axis = 1).round(3)\n",
    "table4.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table4 = table4.transpose()\n",
    "table4.to_csv(os.path.dirname(os.getcwd()) + '\\\\table4_data.csv', header = True, index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "id": "ec975aca-900c-4d04-98ac-f97aa08e01c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data for CAPM, FF3, and FF5 \n",
    "FF5 = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\F-F_Research_Data_5_Factors_2x3.csv')\n",
    "FF5.rename(columns = {'Unnamed: 0':'date'}, inplace = True)\n",
    "FF5 = FF5[(FF5[\"date\"] >= 198701) & (FF5[\"date\"] < 200301)] # 201701\n",
    "FF5 = FF5.set_index('date')\n",
    "FF5 = FF5[[\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]]\n",
    "\n",
    "# Define FF3 and CAPM from FF5\n",
    "FF3 = FF5[[\"Mkt-RF\", \"SMB\", \"HML\"]]\n",
    "CAPM = FF5[\"Mkt-RF\"]\n",
    "\n",
    "# LR regressions\n",
    "CAPM_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# Lasso regressions\n",
    "CAPM_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# NN regressions\n",
    "CAPM_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# CAPM table\n",
    "CAPM_params, CAPM_tvalues = pred.LM_tables(CAPM_LR, CAPM_lasso, CAPM_NN)\n",
    "CAPM_params.index = [\"Constant\", \"Mkt-RF\"]\n",
    "CAPM_tvalues.index = [\"Constant\", \"Mkt-RF\"]\n",
    "CAPM_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\CAPM_params.csv', header = True, index = True) \n",
    "CAPM_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\CAPM_tvalues.csv', header = True, index = True) \n",
    "\n",
    "# FF3 table\n",
    "FF3_params, FF3_tvalues = pred.LM_tables(FF3_LR, FF3_lasso, FF3_NN)\n",
    "FF3_params.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\"]\n",
    "FF3_tvalues.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\"]\n",
    "FF3_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF3_params.csv', header = True, index = True) \n",
    "FF3_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF3_tvalues.csv', header = True, index = True) \n",
    "\n",
    "# FF5 table \n",
    "FF5_params, FF5_tvalues = pred.LM_tables(FF5_LR, FF5_lasso, FF5_NN)\n",
    "FF5_params.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "FF5_tvalues.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "FF5_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF5_params.csv', header = True, index = True) \n",
    "FF5_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF5_tvalues.csv', header = True, index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "id": "2494feea-0765-460b-9e49-2ed2c1d3e2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    ret   R-squared:                       0.083\n",
      "Model:                            OLS   Adj. R-squared:                  0.068\n",
      "Method:                 Least Squares   F-statistic:                     5.636\n",
      "Date:                Sat, 03 Dec 2022   Prob (F-statistic):            0.00102\n",
      "Time:                        19:30:58   Log-Likelihood:                -620.64\n",
      "No. Observations:                 192   AIC:                             1249.\n",
      "Df Residuals:                     188   BIC:                             1262.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.0186      0.459      4.402      0.000       1.114       2.923\n",
      "x1             0.3403      0.108      3.148      0.002       0.127       0.553\n",
      "x2             0.0995      0.134      0.743      0.458      -0.165       0.364\n",
      "x3             0.6140      0.161      3.815      0.000       0.296       0.931\n",
      "==============================================================================\n",
      "Omnibus:                       55.293   Durbin-Watson:                   1.905\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1352.794\n",
      "Skew:                           0.261   Prob(JB):                    1.76e-294\n",
      "Kurtosis:                      15.993   Cond. No.                         5.33\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(FF3_LR.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78618e90-443a-4ca2-9a69-914ee0e2f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skal jeg lave en normals OLS på ikke PCA data? bare som totalt benchmark? -- køre det på FM_data + industry codes så -- ingen interaktion terms eller PCA "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
