{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de3de21-6556-4f1e-80e2-80a89e701949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load dependencies\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import itertools as it\n",
    "import DataProcessFunctions as DP\n",
    "import PredictionStep1 as pred\n",
    "import SupportFunctions as supp\n",
    "import LinearModelEstimation as lm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import statsmodels.api as sm  \n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import callbacks\n",
    "from matplotlib.pyplot import cm\n",
    "import time as time \n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Disable scientific notation (e) in numpy\n",
    "# Disable futurewarnings\n",
    "np.set_printoptions(suppress=True)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8f707-f8b3-4490-85d4-9463d4525581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize warning log container \n",
    "log = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76be5616-1015-4f3d-b1bc-de5deed9e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before downcast: 1.769 GB and float64    102\n",
      "int64        3\n",
      "dtype: int64\n",
      "After downcast: 0.878 GB and float32    102\n",
      "int32        2\n",
      "int8         1\n",
      "dtype: int64\n",
      "Before downcast: 0.026 GB and float64    1\n",
      "dtype: int64\n",
      "After downcast: 0.018 GB and float32    1\n",
      "dtype: int64\n",
      "Before downcast: 0.026 GB and float64    1\n",
      "dtype: int64\n",
      "After downcast: 0.018 GB and float32    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load Pre-processed data from Data Processing PCA.ipynb\n",
    "FM_data = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\FM2_data.csv')\n",
    "returns = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\returns2_data.csv').set_index([\"permno\", \"date\"])\n",
    "industry_code = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\industry2_codes.csv').set_index([\"permno\", \"date\"])\n",
    "\n",
    "# Load excess SP500 log returns data \n",
    "SP500 = pd.read_excel(os.path.dirname(os.getcwd()) + '\\\\SP500.xlsx')[\"Cum return\"]\n",
    "\n",
    "supp.downcast(FM_data)\n",
    "supp.downcast(returns)\n",
    "supp.downcast(industry_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa87af2c-5685-4f1c-a59f-1cbcb4e35d77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data_processing_new time: 0.19572718938191733\n",
      "interaction_new time: 6.031645385424296 Training Dtypes: float32    765\n",
      "dtype: int64, Validation Dtypes: float32    765\n",
      "dtype: int64, Test Dtypes: float32    765\n",
      "dtype: int64,\n",
      "pca time: 12.750190997123719\n",
      "Rows Training set: 406519, Row Validation set: 629972, Rows test set: 63913, Columns: 168, Iteration finished: 0, Time: 19.070224698384603, \n",
      "1998/1998 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.25284357865651447\n",
      "interaction_new time: 6.482802565892538 Training Dtypes: float32    792\n",
      "dtype: int64, Validation Dtypes: float32    792\n",
      "dtype: int64, Test Dtypes: float32    792\n",
      "dtype: int64,\n",
      "pca time: 15.559682019551595\n",
      "Rows Training set: 444575, Row Validation set: 644834, Rows test set: 63116, Columns: 166, Iteration finished: 1, Time: 22.39165099064509, \n",
      "1973/1973 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.2937096079190572\n",
      "interaction_new time: 6.621692641576131 Training Dtypes: float32    792\n",
      "dtype: int64, Validation Dtypes: float32    792\n",
      "dtype: int64, Test Dtypes: float32    792\n",
      "dtype: int64,\n",
      "pca time: 16.93318593899409\n",
      "Rows Training set: 482843, Row Validation set: 658706, Rows test set: 59846, Columns: 164, Iteration finished: 2, Time: 23.989849563439687, \n",
      "1871/1871 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.35150587956110635\n",
      "interaction_new time: 7.124973448117574 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 18.770783535639445\n",
      "Rows Training set: 521296, Row Validation set: 669087, Rows test set: 57942, Columns: 164, Iteration finished: 3, Time: 26.369913538297016, \n",
      "1811/1811 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.3724203189214071\n",
      "interaction_new time: 7.207235542933146 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 20.286175553003947\n",
      "Rows Training set: 558106, Row Validation set: 678900, Rows test set: 56180, Columns: 163, Iteration finished: 4, Time: 27.992033267021178, \n",
      "1756/1756 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.4182912588119507\n",
      "interaction_new time: 7.5435129801432295 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 21.49121822118759\n",
      "Rows Training set: 594100, Row Validation set: 687598, Rows test set: 56596, Columns: 168, Iteration finished: 5, Time: 29.602479334672292, \n",
      "1769/1769 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.4282512863477071\n",
      "interaction_new time: 7.84084860086441 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 22.81368798414866\n",
      "Rows Training set: 626310, Row Validation set: 696363, Rows test set: 58663, Columns: 171, Iteration finished: 6, Time: 31.22390031417211, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.488e+00, tolerance: 1.362e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1834/1834 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.4531011819839478\n",
      "interaction_new time: 8.2649666984876 Training Dtypes: float32    819\n",
      "dtype: int64, Validation Dtypes: float32    819\n",
      "dtype: int64, Test Dtypes: float32    819\n",
      "dtype: int64,\n",
      "pca time: 25.03092161814372\n",
      "Rows Training set: 656279, Row Validation set: 703774, Rows test set: 63861, Columns: 178, Iteration finished: 7, Time: 33.90625932614009, \n",
      "1996/1996 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.49604164361953734\n",
      "interaction_new time: 8.199526115258534 Training Dtypes: float32    819\n",
      "dtype: int64, Validation Dtypes: float32    819\n",
      "dtype: int64, Test Dtypes: float32    819\n",
      "dtype: int64,\n",
      "pca time: 26.45335183143616\n",
      "Rows Training set: 687364, Row Validation set: 714836, Rows test set: 65068, Columns: 176, Iteration finished: 8, Time: 35.29411912759145, \n",
      "2034/2034 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.5292141954104106\n",
      "interaction_new time: 8.629016864299775 Training Dtypes: float32    819\n",
      "dtype: int64, Validation Dtypes: float32    819\n",
      "dtype: int64, Test Dtypes: float32    819\n",
      "dtype: int64,\n",
      "pca time: 27.704849807421365\n",
      "Rows Training set: 719916, Row Validation set: 725226, Rows test set: 68286, Columns: 172, Iteration finished: 9, Time: 37.0187957406044, \n",
      "2134/2134 [==============================] - 5s 2ms/step\n",
      " data_processing_new time: 0.5539716283480326\n",
      "interaction_new time: 9.055329271157582 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 28.663938001791635\n",
      "Rows Training set: 757867, Row Validation set: 733249, Rows test set: 70136, Columns: 173, Iteration finished: 10, Time: 38.42384855349859, \n",
      "2192/2192 [==============================] - 5s 2ms/step\n",
      " data_processing_new time: 0.5865251302719117\n",
      "interaction_new time: 8.980621294180553 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 30.036812317371368\n",
      "Rows Training set: 795114, Row Validation set: 743801, Rows test set: 66870, Columns: 180, Iteration finished: 11, Time: 39.77269336382548, \n",
      "2090/2090 [==============================] - 5s 2ms/step\n",
      " data_processing_new time: 0.6197959740956624\n",
      "interaction_new time: 9.616305470466614 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 31.52532688776652\n",
      "Rows Training set: 833290, Row Validation set: 750477, Rows test set: 60191, Columns: 181, Iteration finished: 12, Time: 41.9297473748525, \n",
      "1881/1881 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.6538205862045288\n",
      "interaction_new time: 10.1439360221227 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 33.27808286746343\n",
      "Rows Training set: 874593, Row Validation set: 746755, Rows test set: 56328, Columns: 193, Iteration finished: 13, Time: 44.26149941682816, \n",
      "1761/1761 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.6991627931594848\n",
      "interaction_new time: 10.301232596238455 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 34.66908071438471\n",
      "Rows Training set: 914136, Row Validation set: 739967, Rows test set: 48683, Columns: 193, Iteration finished: 14, Time: 45.89162456989288, \n",
      "1522/1522 [==============================] - 3s 2ms/step\n",
      " data_processing_new time: 0.7106883843739827\n",
      "interaction_new time: 10.367021735509237 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 36.148236926396685\n",
      "Rows Training set: 949773, Row Validation set: 728804, Rows test set: 41635, Columns: 196, Iteration finished: 15, Time: 47.419610329469045, \n",
      "1302/1302 [==============================] - 3s 2ms/step\n",
      " data_processing_new time: 0.728712546825409\n",
      "interaction_new time: 9.935686361789703 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 38.25092307329178\n",
      "Rows Training set: 982084, Row Validation set: 712497, Rows test set: 36376, Columns: 195, Iteration finished: 16, Time: 49.106200432777406, \n",
      "1137/1137 [==============================] - 3s 2ms/step\n",
      " data_processing_new time: 0.7309221108754476\n",
      "interaction_new time: 10.213100930054983 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 37.16559949715932\n",
      "Rows Training set: 982081, Row Validation set: 692693, Rows test set: 33523, Columns: 194, Iteration finished: 17, Time: 48.29100670814514, \n",
      "1048/1048 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7313813805580139\n",
      "interaction_new time: 10.242483011881511 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 37.22451796134313\n",
      "Rows Training set: 987565, Row Validation set: 669620, Rows test set: 31616, Columns: 195, Iteration finished: 18, Time: 48.37679163217545, \n",
      "988/988 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7350185116132101\n",
      "interaction_new time: 9.788363615671793 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 37.767355704307555\n",
      "Rows Training set: 997177, Row Validation set: 642573, Rows test set: 29593, Columns: 194, Iteration finished: 19, Time: 48.47844264507294, \n",
      "925/925 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7472461740175883\n",
      "interaction_new time: 8.996879700819651 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 38.00953181584676\n",
      "Rows Training set: 1011794, Row Validation set: 608305, Rows test set: 27099, Columns: 195, Iteration finished: 20, Time: 47.92828919490179, \n",
      "847/847 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7510241309801737\n",
      "interaction_new time: 8.791736356417339 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 39.11580282052358\n",
      "Rows Training set: 1027397, Row Validation set: 570336, Rows test set: 24154, Columns: 195, Iteration finished: 21, Time: 48.843015388647714, \n",
      "755/755 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7577765425046284\n",
      "interaction_new time: 8.278786555926006 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 39.51939038038254\n",
      "Rows Training set: 1047554, Row Validation set: 526204, Rows test set: 20111, Columns: 192, Iteration finished: 22, Time: 48.72426248391469, \n",
      "629/629 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7703364133834839\n",
      "interaction_new time: 8.186699700355529 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 40.496753942966464\n",
      "Rows Training set: 1070208, Row Validation set: 476179, Rows test set: 17220, Columns: 190, Iteration finished: 23, Time: 49.61691217422485, \n",
      "539/539 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7714166800181071\n",
      "interaction_new time: 7.4904886245727536 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 40.998188141981764\n",
      "Rows Training set: 1089247, Row Validation set: 426529, Rows test set: 14187, Columns: 188, Iteration finished: 24, Time: 49.41256016095479, \n",
      "444/444 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7564564983050028\n",
      "interaction_new time: 6.659234587351481 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 41.19892533222834\n",
      "Rows Training set: 1098186, Row Validation set: 380525, Rows test set: 11413, Columns: 185, Iteration finished: 25, Time: 48.76110103527705, \n",
      "357/357 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7665914456049602\n",
      "interaction_new time: 6.662136137485504 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 41.537047044436136\n",
      "Rows Training set: 1101715, Row Validation set: 335610, Rows test set: 8917, Columns: 185, Iteration finished: 26, Time: 49.156575878461204, \n",
      "279/279 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7718949476877849\n",
      "interaction_new time: 6.418417696158091 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 45.059520200888315\n",
      "Rows Training set: 1095720, Row Validation set: 295844, Rows test set: 7019, Columns: 181, Iteration finished: 27, Time: 52.394247285525005, \n",
      "220/220 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7826201359430949\n",
      "interaction_new time: 6.202774945894877 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 41.3652109225591\n",
      "Rows Training set: 1077092, Row Validation set: 261228, Rows test set: 4798, Columns: 173, Iteration finished: 28, Time: 48.505708305041, \n",
      "150/150 [==============================] - 0s 2ms/step\n",
      " data_processing_new time: 0.7231224417686463\n",
      "interaction_new time: 5.013812935352325 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 40.95956143140793\n",
      "Rows Training set: 1053884, Row Validation set: 229650, Rows test set: 1597, Columns: 161, Iteration finished: 29, Time: 46.849980314572655, \n",
      "50/50 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Set number of iterations\n",
    "ite = 30 # 30 splits --> iteration = [0;29]\n",
    "\n",
    "# Initialize arrays to store loss and explained variation\n",
    "loss = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "xplained_variation = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "\n",
    "# Initialize arrays to store annual loss and explained variation\n",
    "loss_annual = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "xplained_variation_annual = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "\n",
    "# Initialize arrays to store predicted and actual returns used in portfolio sorts later\n",
    "LR_pred_actual = pd.DataFrame()\n",
    "lasso_pred_actual = pd.DataFrame()\n",
    "NN_pred_actual = pd.DataFrame()\n",
    "\n",
    "for i in range(ite):\n",
    "    \n",
    "    # Compute training, validation, and test set\n",
    "    training, validation, test = DP.complete_data_process(industry_code, returns, FM_data, iteration = i)\n",
    "    \n",
    "    # Split in X and Y\n",
    "    training_x, training_y = pred.XY_split(training)\n",
    "    validation_x, validation_y = pred.XY_split(validation)\n",
    "    test_x, test_y = pred.XY_split(test)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # Algorithm 1: Simple Linear (PCR in Gu, kelly, and Xiu (2020) due to PCA)\n",
    "    # Fit model on training set & predict on test set\n",
    "    LR = lr().fit(training_x, training_y)\n",
    "    LR_pred = LR.predict(test_x)\n",
    "    \n",
    "    # Algoritm 1: Compute loss and explained varation, and combine \n",
    "    # actual and predicted returns. Append all. \n",
    "    LR_loss, LR_explained_var, LR_pred_actual_temp, LR_loss_annual, LR_explained_var_annual = pred.to_append(LR_pred, test_y)\n",
    "    loss.iloc[0, i] = LR_loss\n",
    "    loss_annual.iloc[0, i] = LR_loss_annual\n",
    "    xplained_variation.iloc[0, i] = LR_explained_var\n",
    "    xplained_variation_annual.iloc[0, i] = LR_explained_var_annual\n",
    "    LR_pred_actual = LR_pred_actual.append(LR_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # ALgorithm 2: LASSO\n",
    "    # Fit model on training set and select tuning parameter based on validation set\n",
    "    lambda_grid = pred.lambda_grid(training_x, training_y)\n",
    "    loss_validation = []\n",
    "\n",
    "    for lamb in lambda_grid:\n",
    "        lasso = Lasso(alpha = lamb, tol = 0.001).fit(training_x, training_y)\n",
    "        lasso_pred = lasso.predict(validation_x)\n",
    "        loss_validation.append(pred.loss_function(lasso_pred, validation_y))\n",
    "\n",
    "    # Fit model with error minimizing tuning parameter\n",
    "    lambda_min = lambda_grid[loss_validation.index(min(loss_validation))]\n",
    "    lasso_min = Lasso(alpha = lambda_min).fit(training_x, training_y)\n",
    "    lasso_min_pred = lasso_min.predict(test_x)\n",
    "\n",
    "    # Algorithm 2: Appending \n",
    "    lasso_loss, lasso_explained_var, lasso_pred_actual_temp, lasso_loss_annual, lasso_explained_var_annual = pred.to_append(lasso_min_pred, test_y)\n",
    "    loss.iloc[1, i] = lasso_loss\n",
    "    loss_annual.iloc[1, i] = lasso_loss_annual\n",
    "    xplained_variation.iloc[1, i] = lasso_explained_var\n",
    "    xplained_variation_annual.iloc[1, i] = lasso_explained_var_annual\n",
    "    lasso_pred_actual = lasso_pred_actual.append(lasso_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # ALgorithm 3: NN \n",
    "    \n",
    "    # Build NN architecture (L1 regularization and batch normalization)\n",
    "    model = pred.NN(training_x)\n",
    "\n",
    "    # Define callback for early stopping\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(training_x, training_y, epochs = 50, batch_size = 1000, verbose = 0, validation_data = (validation_x, validation_y), callbacks = [callback])\n",
    "\n",
    "    # Compute predictions\n",
    "    NN_pred = model.predict(test_x)\n",
    "    \n",
    "    # Algorithm 3: Appending\n",
    "    NN_loss, NN_explained_var, NN_pred_actual_temp, NN_loss_annual, NN_explained_var_annual = pred.to_append(NN_pred, test_y)\n",
    "    loss.iloc[2, i] = NN_loss\n",
    "    loss_annual.iloc[2, i] = NN_loss_annual\n",
    "    xplained_variation.iloc[2, i] = NN_explained_var\n",
    "    xplained_variation_annual.iloc[2, i] = NN_explained_var_annual\n",
    "    NN_pred_actual = NN_pred_actual.append(NN_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # Free memory\n",
    "    del training\n",
    "    del validation\n",
    "    del test\n",
    "    del training_x\n",
    "    del training_y\n",
    "    del validation_x\n",
    "    del validation_y\n",
    "    del test_x\n",
    "    del test_y\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b3182e-0172-443c-99ff-07d7ebe8bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "loss.to_csv(os.path.dirname(os.getcwd()) + '\\\\loss.csv', header = True, index = True)\n",
    "xplained_variation.to_csv(os.path.dirname(os.getcwd()) + '\\\\explained_variation.csv', header = True, index = True)\n",
    "loss_annual.to_csv(os.path.dirname(os.getcwd()) + '\\\\loss_annual.csv', header = True, index = True)\n",
    "xplained_variation_annual.to_csv(os.path.dirname(os.getcwd()) + '\\\\explained_variation_annual.csv', header = True, index = True)\n",
    "\n",
    "LR_pred_actual.to_csv(os.path.dirname(os.getcwd()) + '\\\\LR_predictions.csv', header = True, index = False)\n",
    "lasso_pred_actual.to_csv(os.path.dirname(os.getcwd()) + '\\\\lasso_predictions.csv', header = True, index = False)\n",
    "NN_pred_actual.to_csv(os.path.dirname(os.getcwd()) + '\\\\NN_predictions.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9165c412-12a1-4c61-bf20-f2c5c1a075ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "loss = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\loss.csv', index_col = 0)\n",
    "xplained_variation =  pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\explained_variation.csv', index_col = 0)\n",
    "LR_pred_actual = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\LR_predictions.csv')\n",
    "lasso_pred_actual = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\lasso_predictions.csv')\n",
    "NN_pred_actual = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\NN_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30cebda2-23ee-451e-88ca-65880e15695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the single observation in december of 2016\n",
    "LR_pred_actual = LR_pred_actual.iloc[:-1, :]\n",
    "lasso_pred_actual = lasso_pred_actual.iloc[:-1, :]\n",
    "NN_pred_actual = NN_pred_actual.iloc[:-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbb4a794-fcf2-4e31-ba2f-9ad89c67f99d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Return for each decile at all points in time\n",
    "LR_deciles_ret = pred.portfolio_sorts_1(LR_pred_actual)\n",
    "lasso_deciles_ret = pred.portfolio_sorts_1(lasso_pred_actual)\n",
    "NN_deciles_ret = pred.portfolio_sorts_1(NN_pred_actual)\n",
    "\n",
    "# Return and prediction for 10-1 portfolios at all points in time\n",
    "LR_10_1_ret = pred.decile_10_1(LR_deciles_ret, log = False)\n",
    "lasso_10_1_ret = pred.decile_10_1(lasso_deciles_ret, log = False)\n",
    "NN_10_1_ret = pred.decile_10_1(NN_deciles_ret, log = False)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Log returns of decile portfolios (cannot accumulate returns in pct.)\n",
    "LR_deciles_log_ret = LR_deciles_ret.copy()\n",
    "LR_deciles_log_ret.ret = np.log(1 + LR_deciles_log_ret.ret)\n",
    "LR_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "lasso_deciles_log_ret = lasso_deciles_ret.copy()\n",
    "lasso_deciles_log_ret.ret = np.log(1 + lasso_deciles_log_ret.ret)\n",
    "lasso_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "NN_deciles_log_ret = NN_deciles_ret.copy()\n",
    "NN_deciles_log_ret.ret = np.log(1 + NN_deciles_log_ret.ret)\n",
    "NN_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Log returns of 10-1 portfolio\n",
    "LR_10_1_log_ret = pred.decile_10_1(LR_deciles_log_ret, log = True)\n",
    "lasso_10_1_log_ret = pred.decile_10_1(lasso_deciles_log_ret, log = True)\n",
    "NN_10_1_log_ret = pred.decile_10_1(NN_deciles_log_ret, log = True)\n",
    "\n",
    "'''\n",
    "# Old incorrect way of computing log returns of 10-1 portfolio\n",
    "# I mistakenly subtract the returns and then take the log\n",
    "# I ought to take the log and then subtract \n",
    "# Log returns of 10-1 portfolio\n",
    "LR_10_1_log_ret = LR_10_1_ret.copy()\n",
    "LR_10_1_log_ret.ret = np.log(1 + LR_10_1_log_ret.ret)\n",
    "LR_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "lasso_10_1_log_ret = lasso_10_1_ret.copy()\n",
    "lasso_10_1_log_ret.ret = np.log(1 + lasso_10_1_log_ret.ret)\n",
    "lasso_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "NN_10_1_log_ret = NN_10_1_ret.copy()\n",
    "NN_10_1_log_ret.ret = np.log(1 + NN_10_1_log_ret.ret)\n",
    "NN_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "'''\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Accumualtive log returns and predicted returns \n",
    "# of all deciles (cannot accumulate returns in pct.)\n",
    "LR_cum_log_ret = pred.portfolio_sorts_acc_return(LR_deciles_log_ret)\n",
    "lasso_cum_log_ret = pred.portfolio_sorts_acc_return(lasso_deciles_log_ret)\n",
    "NN_cum_log_ret = pred.portfolio_sorts_acc_return(NN_deciles_log_ret)\n",
    "\n",
    "# Monthly average return and std. deviation, and annualized SR\n",
    "# of both predicted and actual returns\n",
    "LR_mean, LR_std, LR_sr = pred.portfolio_sorts_SR(LR_deciles_ret)\n",
    "lasso_mean, lasso_std, lasso_sr = pred.portfolio_sorts_SR(lasso_deciles_ret)\n",
    "NN_mean, NN_std, NN_sr = pred.portfolio_sorts_SR(NN_deciles_ret)\n",
    "\n",
    "# Monthly average return and std. deviation, and annualized SR \n",
    "# for 10-1 portfolio\n",
    "LR_10_1_mean, LR_10_1_sd, LR_10_1_sr = pred.portfolio_sorts_SR(LR_10_1_ret)\n",
    "lasso_10_1_mean, lasso_10_1_sd, lasso_10_1_sr = pred.portfolio_sorts_SR(lasso_10_1_ret)\n",
    "NN_10_1_mean, NN_10_1_sd, NN_10_1_sr = pred.portfolio_sorts_SR(NN_10_1_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2165937-8930-4094-b3c3-d3d4c2d02f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of all deciles for each ML model\n",
    "pred.cumulative_ret_fig(data = LR_deciles_log_ret, name = \"LR_cumulative_log_ret\", market = SP500, save_fig = False, hide = True)\n",
    "pred.cumulative_ret_fig(data = lasso_deciles_log_ret, name = \"lasso_cumulative_log_ret\", market = SP500, save_fig = False, hide = True)\n",
    "pred.cumulative_ret_fig(data = NN_deciles_log_ret, name = \"NN_cumulative_log_ret\", market = SP500, save_fig = False, hide = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1fa70c0-0912-4900-aa9b-a92889f5092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of 10-1 portfolio for each ML model\n",
    "pred.portfolio_10_1_fig(name = \"10-1_portfolio_cumulative_log_ret\", market = SP500, save_fig = False, hide = True, data1 = LR_10_1_log_ret, data2 = lasso_10_1_log_ret, data3 = NN_10_1_log_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "adc7fff4-1c51-49db-b767-eae67bb7437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of 1st and 10th decile of specified ML models \n",
    "pred.deciles_10_1_fig(name = \"deciles_10_1_cumulative_log_ret\", market = SP500, save_fig = True, hide = True, data1 = LR_deciles_log_ret, data2 = lasso_deciles_log_ret, data3 = NN_deciles_log_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ea367d1-d611-4efc-b97e-e854d14dcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for export to R so as to customize for tables \n",
    "\n",
    "# Table 1: monthly loss and explained variation (Multiply with 100 to get pct.)\n",
    "monthly_pricing_error = loss.mean(axis = 1) * 100\n",
    "monthly_xplained_variation = xplained_variation.mean(axis = 1) * 100\n",
    "table1 = pd.concat([monthly_pricing_error, monthly_xplained_variation], axis = 1)\n",
    "table1.columns = [\"Squared Pricing Error\", \"Explained Variation\"]\n",
    "table1 = table1.transpose()\n",
    "table1.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table1.to_csv(os.path.dirname(os.getcwd()) + '\\\\table1_data.csv', header = True, index = True)\n",
    "\n",
    "''' Incorrect as is. Consult Predictionstep1.py, to_append function\n",
    "# Table 2: Annual loss and explained variation (Multiply with 100 to get pct.)\n",
    "annual_pricing_error = loss_annual.mean(axis = 1) * 100\n",
    "annual_xplained_variation = xplained_variation_annual.mean(axis = 1) * 100\n",
    "table2 = pd.concat([annual_pricing_error, annual_xplained_variation], axis = 1)\n",
    "table2.columns = [\"Squared Pricing Error\", \"Explained Variation\"]\n",
    "table2 = table2.transpose()\n",
    "table2.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table2.to_csv(os.path.dirname(os.getcwd()) + '\\\\table2_data.csv', header = True, index = True)\n",
    "'''\n",
    "\n",
    "# Table 3: (Multiply with 100 to get pct.)\n",
    "table3_LR = pd.concat([LR_mean * 100, LR_std.ret * 100, LR_sr.ret], axis = 1).round(2)\n",
    "table3_LR.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_lasso = pd.concat([lasso_mean * 100 , lasso_std.ret * 100, lasso_sr.ret], axis = 1).round(2)\n",
    "table3_lasso.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_NN = pd.concat([NN_mean * 100, NN_std.ret * 100, NN_sr.ret], axis = 1).round(2)\n",
    "table3_NN.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "\n",
    "# Table 3: Add 10-1 portfolio row for each ML model\n",
    "LR_portfolio_10_1_row = pd.concat((LR_10_1_mean * 100, LR_10_1_sd.ret * 100, LR_10_1_sr.ret), axis = 1)\n",
    "LR_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_LR = pd.concat((table3_LR, LR_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "lasso_portfolio_10_1_row = pd.concat((lasso_10_1_mean * 100, lasso_10_1_sd.ret * 100, lasso_10_1_sr.ret), axis = 1)\n",
    "lasso_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_lasso = pd.concat((table3_lasso, lasso_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "NN_portfolio_10_1_row = pd.concat((NN_10_1_mean * 100, NN_10_1_sd.ret * 100, NN_10_1_sr.ret), axis = 1)\n",
    "NN_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_NN = pd.concat((table3_NN, NN_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "# Table 4: Tabel for appendix figurerne (skal bruge LR_cum, lasso_cum, NN_cum) og så bare kun ret søjlen. Har så tabel der viser end point for figurene (denne tabel kommer i appendix) \n",
    "table4 = pd.concat([LR_cum_log_ret.log_ret, lasso_cum_log_ret.log_ret, NN_cum_log_ret.log_ret], axis = 1).round(3)\n",
    "table4.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table4 = table4.transpose()\n",
    "table4.to_csv(os.path.dirname(os.getcwd()) + '\\\\table4_data.csv', header = True, index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ec975aca-900c-4d04-98ac-f97aa08e01c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data for CAPM, FF3, and FF5 \n",
    "FF5 = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\F-F_Research_Data_5_Factors_2x3.csv')\n",
    "FF5.rename(columns = {'Unnamed: 0':'date'}, inplace = True)\n",
    "FF5 = FF5[(FF5[\"date\"] >= 198701) & (FF5[\"date\"] < 201612)]\n",
    "FF5 = FF5.set_index('date')\n",
    "FF5 = FF5[[\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]]\n",
    "\n",
    "# Define FF3 and CAPM from FF5\n",
    "FF3 = FF5[[\"Mkt-RF\", \"SMB\", \"HML\"]]\n",
    "CAPM = FF5[\"Mkt-RF\"]\n",
    "\n",
    "# LR regressions\n",
    "CAPM_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# Lasso regressions\n",
    "CAPM_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# NN regressions\n",
    "CAPM_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# CAPM table\n",
    "CAPM_params, CAPM_tvalues = pred.LM_tables(CAPM_LR, CAPM_lasso, CAPM_NN)\n",
    "CAPM_params.index = [\"Constant\", \"Mkt-RF\"]\n",
    "CAPM_tvalues.index = [\"Constant\", \"Mkt-RF\"]\n",
    "CAPM_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\CAPM_params.csv', header = True, index = True) \n",
    "CAPM_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\CAPM_tvalues.csv', header = True, index = True) \n",
    "\n",
    "# FF3 table\n",
    "FF3_params, FF3_tvalues = pred.LM_tables(FF3_LR, FF3_lasso, FF3_NN)\n",
    "FF3_params.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\"]\n",
    "FF3_tvalues.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\"]\n",
    "FF3_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF3_params.csv', header = True, index = True) \n",
    "FF3_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF3_tvalues.csv', header = True, index = True) \n",
    "\n",
    "# FF5 table \n",
    "FF5_params, FF5_tvalues = pred.LM_tables(FF5_LR, FF5_lasso, FF5_NN)\n",
    "FF5_params.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "FF5_tvalues.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "FF5_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF5_params.csv', header = True, index = True) \n",
    "FF5_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF5_tvalues.csv', header = True, index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bbef2e10-33fd-4105-a960-6c5932978af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(FF3_LR.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78618e90-443a-4ca2-9a69-914ee0e2f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skal jeg lave en normals OLS på ikke PCA data? bare som totalt benchmark? -- køre det på FM_data + industry codes så -- ingen interaktion terms eller PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "863d9d1a-936e-4efb-95e6-710858dd4f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SP = pd.read_excel(os.path.dirname(os.getcwd()) + '\\\\SP500.xlsx')[\"Pct change\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a12d545-50a6-4429-a0cd-7f0e7e08dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "gns = SP.mean()\n",
    "sdafv = SP.std()\n",
    "sharpe = np.sqrt(12) * (gns / sdafv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81d8af80-d34c-40ca-8b2f-fa93e9165d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6531847249023175"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharpe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
