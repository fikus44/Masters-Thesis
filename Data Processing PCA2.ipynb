{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de3de21-6556-4f1e-80e2-80a89e701949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load dependencies\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import itertools as it\n",
    "import DataProcessFunctions as DP\n",
    "import PredictionStep1 as pred\n",
    "import SupportFunctions as supp\n",
    "import LinearModelEstimation as lm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import statsmodels.api as sm  \n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import callbacks\n",
    "from matplotlib.pyplot import cm\n",
    "import time as time \n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Disable scientific notation (e) in numpy\n",
    "# Disable futurewarnings\n",
    "np.set_printoptions(suppress=True)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8f707-f8b3-4490-85d4-9463d4525581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize warning log container \n",
    "log = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76be5616-1015-4f3d-b1bc-de5deed9e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before downcast: 1.769 GB and float64    102\n",
      "int64        3\n",
      "dtype: int64\n",
      "After downcast: 0.878 GB and float32    102\n",
      "int32        2\n",
      "int8         1\n",
      "dtype: int64\n",
      "Before downcast: 0.026 GB and float64    1\n",
      "dtype: int64\n",
      "After downcast: 0.018 GB and float32    1\n",
      "dtype: int64\n",
      "Before downcast: 0.026 GB and float64    1\n",
      "dtype: int64\n",
      "After downcast: 0.018 GB and float32    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load Pre-processed data from Data Processing PCA.ipynb\n",
    "FM_data = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\FM2_data.csv')\n",
    "returns = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\returns2_data.csv').set_index([\"permno\", \"date\"])\n",
    "industry_code = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\industry2_codes.csv').set_index([\"permno\", \"date\"])\n",
    "\n",
    "# Load excess SP500 log returns data \n",
    "SP500 = pd.read_excel(os.path.dirname(os.getcwd()) + '\\\\SP500.xlsx')[\"Cum return\"]\n",
    "\n",
    "supp.downcast(FM_data)\n",
    "supp.downcast(returns)\n",
    "supp.downcast(industry_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa87af2c-5685-4f1c-a59f-1cbcb4e35d77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data_processing_new time: 0.43802690505981445\n",
      "interaction_new time: 7.435297004381815\n",
      "pca time: 13.762475029627483\n",
      "Rows Training set: 406519, Row Validation set: 629972, Rows test set: 63913, Columns: 168, Iteration finished: 0, Time: 21.7574209690094, Training Dtypes: float32    102\n",
      "uint8       66\n",
      "dtype: int64, Validation Dtypes: float32    102\n",
      "uint8       66\n",
      "dtype: int64, Test Dtypes: float32    102\n",
      "uint8       66\n",
      "dtype: int64,\n",
      "1998/1998 [==============================] - 4s 2ms/step\n",
      " data_processing_new time: 0.26538562774658203\n",
      "interaction_new time: 7.35698523124059\n",
      "pca time: 16.18630340496699\n",
      "Rows Training set: 444575, Row Validation set: 644834, Rows test set: 63116, Columns: 166, Iteration finished: 1, Time: 23.919417476654054, Training Dtypes: float32    99\n",
      "uint8      67\n",
      "dtype: int64, Validation Dtypes: float32    99\n",
      "uint8      67\n",
      "dtype: int64, Test Dtypes: float32    99\n",
      "uint8      67\n",
      "dtype: int64,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lamb \u001b[38;5;129;01min\u001b[39;00m lambda_grid:\n\u001b[0;32m     51\u001b[0m     lasso \u001b[38;5;241m=\u001b[39m Lasso(alpha \u001b[38;5;241m=\u001b[39m lamb, tol \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(training_x, training_y)\n\u001b[1;32m---> 52\u001b[0m     lasso_pred \u001b[38;5;241m=\u001b[39m \u001b[43mlasso\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     loss_validation\u001b[38;5;241m.\u001b[39mappend(pred\u001b[38;5;241m.\u001b[39mloss_function(lasso_pred, validation_y))\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Fit model with error minimizing tuning parameter\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1127\u001b[0m, in \u001b[0;36mElasticNet._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:369\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    367\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 369\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    854\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    860\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py:918\u001b[0m, in \u001b[0;36mDataFrame._values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    916\u001b[0m blocks \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n\u001b[0;32m    920\u001b[0m arr \u001b[38;5;241m=\u001b[39m blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;66;03m# non-2D ExtensionArray\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py:10889\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  10816\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  10817\u001b[0m \u001b[38;5;124;03mReturn a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[0;32m  10818\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10886\u001b[0m \u001b[38;5;124;03m       ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[0;32m  10887\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  10888\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[1;32m> 10889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1589\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1587\u001b[0m             arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1589\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1590\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave\u001b[39;00m\n\u001b[0;32m   1591\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1654\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1652\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m         arr \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mget_values(dtype)\n\u001b[1;32m-> 1654\u001b[0m     \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m   1655\u001b[0m     itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set number of iterations\n",
    "ite = 7 # 30 splits --> iteration = [0;29]\n",
    "\n",
    "# Initialize arrays to store loss and explained variation\n",
    "loss = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "xplained_variation = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "\n",
    "# Initialize arrays to store annual loss and explained variation\n",
    "loss_annual = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "xplained_variation_annual = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "\n",
    "# Initialize arrays to store predicted and actual returns used in portfolio sorts later\n",
    "LR_pred_actual = pd.DataFrame()\n",
    "lasso_pred_actual = pd.DataFrame()\n",
    "NN_pred_actual = pd.DataFrame()\n",
    "\n",
    "for i in range(ite):\n",
    "    \n",
    "    # Compute training, validation, and test set\n",
    "    training, validation, test = DP.complete_data_process(industry_code, returns, FM_data, iteration = i)\n",
    "    \n",
    "    # Split in X and Y\n",
    "    training_x, training_y = pred.XY_split(training)\n",
    "    validation_x, validation_y = pred.XY_split(validation)\n",
    "    test_x, test_y = pred.XY_split(test)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # Algorithm 1: Simple Linear (PCR in Gu, kelly, and Xiu (2020) due to PCA)\n",
    "    # Fit model on training set & predict on test set\n",
    "    LR = lr().fit(training_x, training_y)\n",
    "    LR_pred = LR.predict(test_x)\n",
    "    \n",
    "    # Algoritm 1: Compute loss and explained varation, and combine \n",
    "    # actual and predicted returns. Append all. \n",
    "    LR_loss, LR_explained_var, LR_pred_actual_temp, LR_loss_annual, LR_explained_var_annual = pred.to_append(LR_pred, test_y)\n",
    "    loss.iloc[0, i] = LR_loss\n",
    "    loss_annual.iloc[0, i] = LR_loss_annual\n",
    "    xplained_variation.iloc[0, i] = LR_explained_var\n",
    "    xplained_variation_annual.iloc[0, i] = LR_explained_var_annual\n",
    "    LR_pred_actual = LR_pred_actual.append(LR_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # ALgorithm 2: LASSO\n",
    "    # Fit model on training set and select tuning parameter based on validation set\n",
    "    lambda_grid = pred.lambda_grid(training_x, training_y)\n",
    "    loss_validation = []\n",
    "\n",
    "    for lamb in lambda_grid:\n",
    "        lasso = Lasso(alpha = lamb, tol = 0.001).fit(training_x, training_y)\n",
    "        lasso_pred = lasso.predict(validation_x)\n",
    "        loss_validation.append(pred.loss_function(lasso_pred, validation_y))\n",
    "\n",
    "    # Fit model with error minimizing tuning parameter\n",
    "    lambda_min = lambda_grid[loss_validation.index(min(loss_validation))]\n",
    "    lasso_min = Lasso(alpha = lambda_min).fit(training_x, training_y)\n",
    "    lasso_min_pred = lasso_min.predict(test_x)\n",
    "\n",
    "    # Algorithm 2: Appending \n",
    "    lasso_loss, lasso_explained_var, lasso_pred_actual_temp, lasso_loss_annual, lasso_explained_var_annual = pred.to_append(lasso_min_pred, test_y)\n",
    "    loss.iloc[1, i] = lasso_loss\n",
    "    loss_annual.iloc[1, i] = lasso_loss_annual\n",
    "    xplained_variation.iloc[1, i] = lasso_explained_var\n",
    "    xplained_variation_annual.iloc[1, i] = lasso_explained_var_annual\n",
    "    lasso_pred_actual = lasso_pred_actual.append(lasso_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # ALgorithm 3: NN \n",
    "    # Build NN architecture (L1 regularization and batch normalization)\n",
    "    model = pred.NN(training_x)\n",
    "\n",
    "    # Define callback for early stopping\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(training_x, training_y, epochs = 50, batch_size = 500, verbose = 0, validation_data = (validation_x, validation_y), callbacks = [callback])\n",
    "\n",
    "    # Compute predictions\n",
    "    NN_pred = model.predict(test_x)\n",
    "    \n",
    "    # Algorithm 3: Appending\n",
    "    NN_loss, NN_explained_var, NN_pred_actual_temp, NN_loss_annual, NN_explained_var_annual = pred.to_append(NN_pred, test_y)\n",
    "    loss.iloc[2, i] = NN_loss\n",
    "    loss_annual.iloc[2, i] = NN_loss_annual\n",
    "    xplained_variation.iloc[2, i] = NN_explained_var\n",
    "    xplained_variation_annual.iloc[2, i] = NN_explained_var_annual\n",
    "    NN_pred_actual = NN_pred_actual.append(NN_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # Free memory\n",
    "    del training\n",
    "    del validation\n",
    "    del test\n",
    "    del training_x\n",
    "    del training_y\n",
    "    del validation_x\n",
    "    del validation_y\n",
    "    del test_x\n",
    "    del test_y\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "dbb4a794-fcf2-4e31-ba2f-9ad89c67f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return for each decile at all points in time\n",
    "LR_deciles_ret = pred.portfolio_sorts_1(LR_pred_actual)\n",
    "lasso_deciles_ret = pred.portfolio_sorts_1(lasso_pred_actual)\n",
    "NN_deciles_ret = pred.portfolio_sorts_1(NN_pred_actual)\n",
    "\n",
    "# Return and prediction for 10-1 portfolios at all points in time\n",
    "LR_10_1_ret = pred.decile_10_1(LR_deciles_ret)\n",
    "lasso_10_1_ret = pred.decile_10_1(lasso_deciles_ret)\n",
    "NN_10_1_ret = pred.decile_10_1(NN_deciles_ret)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Log returns of decile portfolios (cannot accumulate returns in pct.)\n",
    "LR_deciles_log_ret = LR_deciles_ret.copy()\n",
    "LR_deciles_log_ret.ret = np.log(1 + LR_deciles_log_ret.ret)\n",
    "LR_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "lasso_deciles_log_ret = lasso_deciles_ret.copy()\n",
    "lasso_deciles_log_ret.ret = np.log(1 + lasso_deciles_log_ret.ret)\n",
    "lasso_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "NN_deciles_log_ret = NN_deciles_ret.copy()\n",
    "NN_deciles_log_ret.ret = np.log(1 + NN_deciles_log_ret.ret)\n",
    "NN_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Log returns of 10-1 portfolio\n",
    "LR_10_1_log_ret = LR_10_1_ret.copy()\n",
    "LR_10_1_log_ret.ret = np.log(1 + LR_10_1_log_ret.ret)\n",
    "LR_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "lasso_10_1_log_ret = lasso_10_1_ret.copy()\n",
    "lasso_10_1_log_ret.ret = np.log(1 + lasso_10_1_log_ret.ret)\n",
    "lasso_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "NN_10_1_log_ret = NN_10_1_ret.copy()\n",
    "NN_10_1_log_ret.ret = np.log(1 + NN_10_1_log_ret.ret)\n",
    "NN_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Accumualtive log returns and predicted returns \n",
    "# of all deciles (cannot accumulate returns in pct.)\n",
    "LR_cum_log_ret = pred.portfolio_sorts_acc_return(LR_deciles_log_ret)\n",
    "lasso_cum_log_ret = pred.portfolio_sorts_acc_return(lasso_deciles_log_ret)\n",
    "NN_cum_log_ret = pred.portfolio_sorts_acc_return(NN_deciles_log_ret)\n",
    "\n",
    "# Monthly average return and std. deviation, and annualized SR\n",
    "# of both predicted and actual returns\n",
    "LR_mean, LR_std, LR_sr = pred.portfolio_sorts_SR(LR_deciles_ret)\n",
    "lasso_mean, lasso_std, lasso_sr = pred.portfolio_sorts_SR(lasso_deciles_ret)\n",
    "NN_mean, NN_std, NN_sr = pred.portfolio_sorts_SR(NN_deciles_ret)\n",
    "\n",
    "# Monthly average return and std. deviation, and annualized SR \n",
    "# for 10-1 portfolio\n",
    "LR_10_1_mean, LR_10_1_sd, LR_10_1_sr = pred.portfolio_sorts_SR(LR_10_1_ret)\n",
    "lasso_10_1_mean, lasso_10_1_sd, lasso_10_1_sr = pred.portfolio_sorts_SR(lasso_10_1_ret)\n",
    "NN_10_1_mean, NN_10_1_sd, NN_10_1_sr = pred.portfolio_sorts_SR(NN_10_1_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "id": "a2165937-8930-4094-b3c3-d3d4c2d02f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of all deciles for each ML model\n",
    "pred.cumulative_ret_fig(data = LR_deciles_log_ret, name = \"LR_cumulative_log_ret\", market = SP500, save_fig = True, hide = True)\n",
    "pred.cumulative_ret_fig(data = lasso_deciles_log_ret, name = \"lasso_cumulative_log_ret\", market = SP500, save_fig = True, hide = True)\n",
    "pred.cumulative_ret_fig(data = NN_deciles_log_ret, name = \"NN_cumulative_log_ret\", market = SP500, save_fig = True, hide = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "b1fa70c0-0912-4900-aa9b-a92889f5092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of 10-1 portfolio for each ML model\n",
    "pred.portfolio_10_1_fig(name = \"10-1_portfolio_cumulative_log_ret\", market = SP500, save_fig = True, hide = True, data1 = LR_10_1_log_ret, data2 = lasso_10_1_log_ret, data3 = NN_10_1_log_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "id": "adc7fff4-1c51-49db-b767-eae67bb7437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of 1st and 10th decile of specified ML models \n",
    "pred.deciles_10_1_fig(name = \"deciles_10_1_cumulative_log_ret\", market = SP500, save_fig = True, hide = True, data1 = LR_deciles_log_ret, data2 = lasso_deciles_log_ret, data3 = NN_deciles_log_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "3ea367d1-d611-4efc-b97e-e854d14dcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for export to R so as to customize for tables \n",
    "\n",
    "# Table 1: monthly loss and explained variation (Multiply with 100 to get pct.)\n",
    "monthly_pricing_error = loss.mean(axis = 1) * 100\n",
    "monthly_xplained_variation = xplained_variation.mean(axis = 1) * 100\n",
    "table1 = pd.concat([monthly_pricing_error, monthly_xplained_variation], axis = 1)\n",
    "table1.columns = [\"Squared Pricing Error\", \"Explained Variation\"]\n",
    "table1 = table1.transpose()\n",
    "table1.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table1.to_csv(os.path.dirname(os.getcwd()) + '\\\\table1_data.csv', header = True, index = True)\n",
    "\n",
    "''' Incorrect as is. Consult Predictionstep1.py, to_append function\n",
    "# Table 2: Annual loss and explained variation (Multiply with 100 to get pct.)\n",
    "annual_pricing_error = loss_annual.mean(axis = 1) * 100\n",
    "annual_xplained_variation = xplained_variation_annual.mean(axis = 1) * 100\n",
    "table2 = pd.concat([annual_pricing_error, annual_xplained_variation], axis = 1)\n",
    "table2.columns = [\"Squared Pricing Error\", \"Explained Variation\"]\n",
    "table2 = table2.transpose()\n",
    "table2.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table2.to_csv(os.path.dirname(os.getcwd()) + '\\\\table2_data.csv', header = True, index = True)\n",
    "'''\n",
    "\n",
    "# Table 3: (Multiply with 100 to get pct.)\n",
    "table3_LR = pd.concat([LR_mean * 100, LR_std.ret * 100, LR_sr.ret], axis = 1).round(2)\n",
    "table3_LR.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_lasso = pd.concat([lasso_mean * 100 , lasso_std.ret * 100, lasso_sr.ret], axis = 1).round(2)\n",
    "table3_lasso.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_NN = pd.concat([NN_mean * 100, NN_std.ret * 100, NN_sr.ret], axis = 1).round(2)\n",
    "table3_NN.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "\n",
    "# Table 3: Add 10-1 portfolio row for each ML model\n",
    "LR_portfolio_10_1_row = pd.concat((LR_10_1_mean * 100, LR_10_1_sd.ret * 100, LR_10_1_sr.ret), axis = 1)\n",
    "LR_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_LR = pd.concat((table3_LR, LR_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "lasso_portfolio_10_1_row = pd.concat((lasso_10_1_mean * 100, lasso_10_1_sd.ret * 100, lasso_10_1_sr.ret), axis = 1)\n",
    "lasso_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_lasso = pd.concat((table3_lasso, lasso_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "NN_portfolio_10_1_row = pd.concat((NN_10_1_mean * 100, NN_10_1_sd.ret * 100, NN_10_1_sr.ret), axis = 1)\n",
    "NN_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_NN = pd.concat((table3_NN, NN_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "# Table 4: Tabel for appendix figurerne (skal bruge LR_cum, lasso_cum, NN_cum) og så bare kun ret søjlen. Har så tabel der viser end point for figurene (denne tabel kommer i appendix) \n",
    "table4 = pd.concat([LR_cum_log_ret.log_ret, lasso_cum_log_ret.log_ret, NN_cum_log_ret.log_ret], axis = 1).round(3)\n",
    "table4.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table4 = table4.transpose()\n",
    "table4.to_csv(os.path.dirname(os.getcwd()) + '\\\\table4_data.csv', header = True, index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "id": "ec975aca-900c-4d04-98ac-f97aa08e01c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data for CAPM, FF3, and FF5 \n",
    "FF5 = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\F-F_Research_Data_5_Factors_2x3.csv')\n",
    "FF5.rename(columns = {'Unnamed: 0':'date'}, inplace = True)\n",
    "FF5 = FF5[(FF5[\"date\"] >= 198701) & (FF5[\"date\"] < 200301)] # 201701\n",
    "FF5 = FF5.set_index('date')\n",
    "FF5 = FF5[[\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]]\n",
    "\n",
    "# Define FF3 and CAPM from FF5\n",
    "FF3 = FF5[[\"Mkt-RF\", \"SMB\", \"HML\"]]\n",
    "CAPM = FF5[\"Mkt-RF\"]\n",
    "\n",
    "# LR regressions\n",
    "CAPM_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# Lasso regressions\n",
    "CAPM_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# NN regressions\n",
    "CAPM_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# CAPM table\n",
    "CAPM_params, CAPM_tvalues = pred.LM_tables(CAPM_LR, CAPM_lasso, CAPM_NN)\n",
    "CAPM_params.index = [\"Constant\", \"Mkt-RF\"]\n",
    "CAPM_tvalues.index = [\"Constant\", \"Mkt-RF\"]\n",
    "CAPM_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\CAPM_params.csv', header = True, index = True) \n",
    "CAPM_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\CAPM_tvalues.csv', header = True, index = True) \n",
    "\n",
    "# FF3 table\n",
    "FF3_params, FF3_tvalues = pred.LM_tables(FF3_LR, FF3_lasso, FF3_NN)\n",
    "FF3_params.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\"]\n",
    "FF3_tvalues.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\"]\n",
    "FF3_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF3_params.csv', header = True, index = True) \n",
    "FF3_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF3_tvalues.csv', header = True, index = True) \n",
    "\n",
    "# FF5 table \n",
    "FF5_params, FF5_tvalues = pred.LM_tables(FF5_LR, FF5_lasso, FF5_NN)\n",
    "FF5_params.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "FF5_tvalues.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "FF5_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF5_params.csv', header = True, index = True) \n",
    "FF5_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF5_tvalues.csv', header = True, index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "id": "2494feea-0765-460b-9e49-2ed2c1d3e2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    ret   R-squared:                       0.083\n",
      "Model:                            OLS   Adj. R-squared:                  0.068\n",
      "Method:                 Least Squares   F-statistic:                     5.636\n",
      "Date:                Sat, 03 Dec 2022   Prob (F-statistic):            0.00102\n",
      "Time:                        19:30:58   Log-Likelihood:                -620.64\n",
      "No. Observations:                 192   AIC:                             1249.\n",
      "Df Residuals:                     188   BIC:                             1262.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.0186      0.459      4.402      0.000       1.114       2.923\n",
      "x1             0.3403      0.108      3.148      0.002       0.127       0.553\n",
      "x2             0.0995      0.134      0.743      0.458      -0.165       0.364\n",
      "x3             0.6140      0.161      3.815      0.000       0.296       0.931\n",
      "==============================================================================\n",
      "Omnibus:                       55.293   Durbin-Watson:                   1.905\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1352.794\n",
      "Skew:                           0.261   Prob(JB):                    1.76e-294\n",
      "Kurtosis:                      15.993   Cond. No.                         5.33\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(FF3_LR.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78618e90-443a-4ca2-9a69-914ee0e2f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skal jeg lave en normals OLS på ikke PCA data? bare som totalt benchmark? -- køre det på FM_data + industry codes så -- ingen interaktion terms eller PCA "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
