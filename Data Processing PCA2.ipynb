{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de3de21-6556-4f1e-80e2-80a89e701949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load dependencies\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import itertools as it\n",
    "import DataProcessFunctions as DP\n",
    "import PredictionStep1 as pred\n",
    "import SupportFunctions as supp\n",
    "import LinearModelEstimation as lm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import statsmodels.api as sm  \n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import callbacks\n",
    "from matplotlib.pyplot import cm\n",
    "import time as time \n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Disable scientific notation (e) in numpy\n",
    "# Disable futurewarnings\n",
    "np.set_printoptions(suppress=True)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8f707-f8b3-4490-85d4-9463d4525581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize warning log container \n",
    "log = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76be5616-1015-4f3d-b1bc-de5deed9e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before downcast: 1.769 GB and float64    102\n",
      "int64        3\n",
      "dtype: int64\n",
      "After downcast: 0.878 GB and float32    102\n",
      "int32        2\n",
      "int8         1\n",
      "dtype: int64\n",
      "Before downcast: 0.026 GB and float64    1\n",
      "dtype: int64\n",
      "After downcast: 0.018 GB and float32    1\n",
      "dtype: int64\n",
      "Before downcast: 0.026 GB and float64    1\n",
      "dtype: int64\n",
      "After downcast: 0.018 GB and float32    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load Pre-processed data from Data Processing PCA.ipynb\n",
    "FM_data = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\FM2_data.csv')\n",
    "returns = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\returns2_data.csv').set_index([\"permno\", \"date\"])\n",
    "industry_code = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\industry2_codes.csv').set_index([\"permno\", \"date\"])\n",
    "\n",
    "# Load excess SP500 log returns data \n",
    "SP500 = pd.read_excel(os.path.dirname(os.getcwd()) + '\\\\SP500.xlsx')[\"Cum return\"]\n",
    "\n",
    "supp.downcast(FM_data)\n",
    "supp.downcast(returns)\n",
    "supp.downcast(industry_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa87af2c-5685-4f1c-a59f-1cbcb4e35d77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data_processing_new time: 0.6868337710698446\n",
      "interaction_new time: 8.627713119983673 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 38.513426605860396\n",
      "Rows Training set: 1011794, Row Validation set: 608305, Rows test set: 27099, Columns: 195, Iteration finished: 20, Time: 47.984201987584434, \n",
      "847/847 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7171700199445089\n",
      "interaction_new time: 8.681127909819285 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 38.48274031082789\n",
      "Rows Training set: 1027397, Row Validation set: 570336, Rows test set: 24154, Columns: 195, Iteration finished: 21, Time: 48.04004948933919, \n",
      "755/755 [==============================] - 2s 2ms/step\n",
      " data_processing_new time: 0.7681570291519165\n",
      "interaction_new time: 8.252127663294475 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 40.79441001812617\n",
      "Rows Training set: 1047554, Row Validation set: 526204, Rows test set: 20111, Columns: 192, Iteration finished: 22, Time: 50.046441260973616, \n",
      "629/629 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7847103476524353\n",
      "interaction_new time: 7.897066613038381 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 40.949536911646526\n",
      "Rows Training set: 1070208, Row Validation set: 476179, Rows test set: 17220, Columns: 190, Iteration finished: 23, Time: 49.80568700234095, \n",
      "539/539 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7937341729799906\n",
      "interaction_new time: 7.3541359027226765 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 41.36046581665675\n",
      "Rows Training set: 1089247, Row Validation set: 426529, Rows test set: 14187, Columns: 188, Iteration finished: 24, Time: 49.6801694393158, \n",
      "444/444 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.8086010257403056\n",
      "interaction_new time: 6.621172106266021 Training Dtypes: float32    801\n",
      "dtype: int64, Validation Dtypes: float32    801\n",
      "dtype: int64, Test Dtypes: float32    801\n",
      "dtype: int64,\n",
      "pca time: 41.21681189139684\n",
      "Rows Training set: 1098186, Row Validation set: 380525, Rows test set: 11413, Columns: 185, Iteration finished: 25, Time: 48.80826689402262, \n",
      "357/357 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7832498709360759\n",
      "interaction_new time: 6.8657629013061525 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 43.17518974542618\n",
      "Rows Training set: 1101715, Row Validation set: 335610, Rows test set: 8917, Columns: 185, Iteration finished: 26, Time: 50.98341998259227, \n",
      "279/279 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7877025326093038\n",
      "interaction_new time: 6.198922149340311 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 42.946260221799214\n",
      "Rows Training set: 1095720, Row Validation set: 295844, Rows test set: 7019, Columns: 181, Iteration finished: 27, Time: 50.13962065776189, \n",
      "220/220 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.7798530141512553\n",
      "interaction_new time: 5.593898403644562 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 42.152706619103746\n",
      "Rows Training set: 1077092, Row Validation set: 261228, Rows test set: 4798, Columns: 173, Iteration finished: 28, Time: 48.66662139495214, \n",
      "150/150 [==============================] - 1s 2ms/step\n",
      " data_processing_new time: 0.740647840499878\n",
      "interaction_new time: 5.170649667580922 Training Dtypes: float32    810\n",
      "dtype: int64, Validation Dtypes: float32    810\n",
      "dtype: int64, Test Dtypes: float32    810\n",
      "dtype: int64,\n",
      "pca time: 39.14104372660319\n",
      "Rows Training set: 1053884, Row Validation set: 229650, Rows test set: 1597, Columns: 161, Iteration finished: 29, Time: 45.18568609158198, \n",
      "50/50 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Set number of iterations\n",
    "ite = 30 # 30 splits --> iteration = [0;29]\n",
    "\n",
    "# Initialize arrays to store loss and explained variation\n",
    "loss = pd.DataFrame(columns = range(ite, 30), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "xplained_variation = pd.DataFrame(columns = range(ite, 30), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "\n",
    "# Initialize arrays to store annual loss and explained variation\n",
    "loss_annual = pd.DataFrame(columns = range(ite, 30), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "xplained_variation_annual = pd.DataFrame(columns = range(ite, 30), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "\n",
    "# Initialize arrays to store predicted and actual returns used in portfolio sorts later\n",
    "LR_pred_actual = pd.DataFrame()\n",
    "lasso_pred_actual = pd.DataFrame()\n",
    "NN_pred_actual = pd.DataFrame()\n",
    "\n",
    "for i in range(ite):\n",
    "    \n",
    "    # Compute training, validation, and test set\n",
    "    training, validation, test = DP.complete_data_process(industry_code, returns, FM_data, iteration = i)\n",
    "    \n",
    "    # Split in X and Y\n",
    "    training_x, training_y = pred.XY_split(training)\n",
    "    validation_x, validation_y = pred.XY_split(validation)\n",
    "    test_x, test_y = pred.XY_split(test)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # Algorithm 1: Simple Linear (PCR in Gu, kelly, and Xiu (2020) due to PCA)\n",
    "    # Fit model on training set & predict on test set\n",
    "    LR = lr().fit(training_x, training_y)\n",
    "    LR_pred = LR.predict(test_x)\n",
    "    \n",
    "    # Algoritm 1: Compute loss and explained varation, and combine \n",
    "    # actual and predicted returns. Append all. \n",
    "    LR_loss, LR_explained_var, LR_pred_actual_temp, LR_loss_annual, LR_explained_var_annual = pred.to_append(LR_pred, test_y)\n",
    "    loss.iloc[0, i] = LR_loss\n",
    "    loss_annual.iloc[0, i] = LR_loss_annual\n",
    "    xplained_variation.iloc[0, i] = LR_explained_var\n",
    "    xplained_variation_annual.iloc[0, i] = LR_explained_var_annual\n",
    "    LR_pred_actual = LR_pred_actual.append(LR_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # ALgorithm 2: LASSO\n",
    "    # Fit model on training set and select tuning parameter based on validation set\n",
    "    lambda_grid = pred.lambda_grid(training_x, training_y)\n",
    "    loss_validation = []\n",
    "\n",
    "    for lamb in lambda_grid:\n",
    "        lasso = Lasso(alpha = lamb, tol = 0.001).fit(training_x, training_y)\n",
    "        lasso_pred = lasso.predict(validation_x)\n",
    "        loss_validation.append(pred.loss_function(lasso_pred, validation_y))\n",
    "\n",
    "    # Fit model with error minimizing tuning parameter\n",
    "    lambda_min = lambda_grid[loss_validation.index(min(loss_validation))]\n",
    "    lasso_min = Lasso(alpha = lambda_min).fit(training_x, training_y)\n",
    "    lasso_min_pred = lasso_min.predict(test_x)\n",
    "\n",
    "    # Algorithm 2: Appending \n",
    "    lasso_loss, lasso_explained_var, lasso_pred_actual_temp, lasso_loss_annual, lasso_explained_var_annual = pred.to_append(lasso_min_pred, test_y)\n",
    "    loss.iloc[1, i] = lasso_loss\n",
    "    loss_annual.iloc[1, i] = lasso_loss_annual\n",
    "    xplained_variation.iloc[1, i] = lasso_explained_var\n",
    "    xplained_variation_annual.iloc[1, i] = lasso_explained_var_annual\n",
    "    lasso_pred_actual = lasso_pred_actual.append(lasso_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # ALgorithm 3: NN \n",
    "    # Build NN architecture (L1 regularization and batch normalization)\n",
    "    model = pred.NN(training_x)\n",
    "\n",
    "    # Define callback for early stopping\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(training_x, training_y, epochs = 50, batch_size = 500, verbose = 0, validation_data = (validation_x, validation_y), callbacks = [callback])\n",
    "\n",
    "    # Compute predictions\n",
    "    NN_pred = model.predict(test_x)\n",
    "    \n",
    "    # Algorithm 3: Appending\n",
    "    NN_loss, NN_explained_var, NN_pred_actual_temp, NN_loss_annual, NN_explained_var_annual = pred.to_append(NN_pred, test_y)\n",
    "    loss.iloc[2, i] = NN_loss\n",
    "    loss_annual.iloc[2, i] = NN_loss_annual\n",
    "    xplained_variation.iloc[2, i] = NN_explained_var\n",
    "    xplained_variation_annual.iloc[2, i] = NN_explained_var_annual\n",
    "    NN_pred_actual = NN_pred_actual.append(NN_pred_actual_temp)\n",
    "    \n",
    "    # ---------------------------\n",
    "    \n",
    "    # Free memory\n",
    "    del training\n",
    "    del validation\n",
    "    del test\n",
    "    del training_x\n",
    "    del training_y\n",
    "    del validation_x\n",
    "    del validation_y\n",
    "    del test_x\n",
    "    del test_y\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60b3182e-0172-443c-99ff-07d7ebe8bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.to_csv(os.path.dirname(os.getcwd()) + '\\\\loss_2.csv', header = True, index = True)\n",
    "xplained_variation.to_csv(os.path.dirname(os.getcwd()) + '\\\\explained_variation_2.csv', header = True, index = True)\n",
    "loss_annual.to_csv(os.path.dirname(os.getcwd()) + '\\\\loss_annual_2.csv', header = True, index = True)\n",
    "xplained_variation_annual.to_csv(os.path.dirname(os.getcwd()) + '\\\\explained_variation_annual_2.csv', header = True, index = True)\n",
    "\n",
    "LR_pred_actual.to_csv(os.path.dirname(os.getcwd()) + '\\\\LR_predictions_2.csv', header = True, index = False)\n",
    "lasso_pred_actual.to_csv(os.path.dirname(os.getcwd()) + '\\\\lasso_predictions_2.csv', header = True, index = False)\n",
    "NN_pred_actual.to_csv(os.path.dirname(os.getcwd()) + '\\\\NN_predictions_2.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9165c412-12a1-4c61-bf20-f2c5c1a075ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_first = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\loss.csv', index_col = 0)\n",
    "xplained_variation_first =  pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\explained_variation.csv', index_col = 0)\n",
    "LR_pred_actual_first = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\LR_predictions.csv')\n",
    "lasso_pred_actual_first = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\lasso_predictions.csv')\n",
    "NN_pred_actual_first = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\NN_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b6e5524-5b6c-4929-9456-6e6a14d82c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = pd.concat((loss_first.iloc[:, :20], loss), axis = 1)\n",
    "xplained_variation = pd.concat((xplained_variation_first.iloc[:, :20], xplained_variation), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f64daeba-da51-4d79-8804-a097dd5013c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_pred_actual = pd.concat((LR_pred_actual_first, LR_pred_actual), axis = 0)\n",
    "lasso_pred_actual = pd.concat((lasso_pred_actual_first, lasso_pred_actual), axis = 0)\n",
    "NN_pred_actual = pd.concat((NN_pred_actual_first, NN_pred_actual), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "30cebda2-23ee-451e-88ca-65880e15695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_pred_actual = LR_pred_actual.iloc[:-1, :]\n",
    "lasso_pred_actual = lasso_pred_actual.iloc[:-1, :]\n",
    "NN_pred_actual = NN_pred_actual.iloc[:-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dbb4a794-fcf2-4e31-ba2f-9ad89c67f99d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\OneDrive - Københavns Universitet\\Documents\\Økonomi - Kandidat\\6. Semester\\Speciale\\Masters-Thesis\\PredictionStep1.py:247: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred_actual[\"rank\"] = pred_actual.groupby(\"date\")[\"pred\"].rank(method = 'first')\n",
      "C:\\Users\\thoma\\OneDrive - Københavns Universitet\\Documents\\Økonomi - Kandidat\\6. Semester\\Speciale\\Masters-Thesis\\PredictionStep1.py:254: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred_actual[\"decile\"] = pred_actual.groupby(\"date\")[\"rank\"].transform(lambda x: pd.qcut(x, 10, labels = range(1, 11)))\n",
      "C:\\Users\\thoma\\OneDrive - Københavns Universitet\\Documents\\Økonomi - Kandidat\\6. Semester\\Speciale\\Masters-Thesis\\PredictionStep1.py:247: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred_actual[\"rank\"] = pred_actual.groupby(\"date\")[\"pred\"].rank(method = 'first')\n",
      "C:\\Users\\thoma\\OneDrive - Københavns Universitet\\Documents\\Økonomi - Kandidat\\6. Semester\\Speciale\\Masters-Thesis\\PredictionStep1.py:254: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred_actual[\"decile\"] = pred_actual.groupby(\"date\")[\"rank\"].transform(lambda x: pd.qcut(x, 10, labels = range(1, 11)))\n",
      "C:\\Users\\thoma\\OneDrive - Københavns Universitet\\Documents\\Økonomi - Kandidat\\6. Semester\\Speciale\\Masters-Thesis\\PredictionStep1.py:247: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred_actual[\"rank\"] = pred_actual.groupby(\"date\")[\"pred\"].rank(method = 'first')\n",
      "C:\\Users\\thoma\\OneDrive - Københavns Universitet\\Documents\\Økonomi - Kandidat\\6. Semester\\Speciale\\Masters-Thesis\\PredictionStep1.py:254: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred_actual[\"decile\"] = pred_actual.groupby(\"date\")[\"rank\"].transform(lambda x: pd.qcut(x, 10, labels = range(1, 11)))\n"
     ]
    }
   ],
   "source": [
    "# Return for each decile at all points in time\n",
    "LR_deciles_ret = pred.portfolio_sorts_1(LR_pred_actual)\n",
    "lasso_deciles_ret = pred.portfolio_sorts_1(lasso_pred_actual)\n",
    "NN_deciles_ret = pred.portfolio_sorts_1(NN_pred_actual)\n",
    "\n",
    "# Return and prediction for 10-1 portfolios at all points in time\n",
    "LR_10_1_ret = pred.decile_10_1(LR_deciles_ret)\n",
    "lasso_10_1_ret = pred.decile_10_1(lasso_deciles_ret)\n",
    "NN_10_1_ret = pred.decile_10_1(NN_deciles_ret)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Log returns of decile portfolios (cannot accumulate returns in pct.)\n",
    "LR_deciles_log_ret = LR_deciles_ret.copy()\n",
    "LR_deciles_log_ret.ret = np.log(1 + LR_deciles_log_ret.ret)\n",
    "LR_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "lasso_deciles_log_ret = lasso_deciles_ret.copy()\n",
    "lasso_deciles_log_ret.ret = np.log(1 + lasso_deciles_log_ret.ret)\n",
    "lasso_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "NN_deciles_log_ret = NN_deciles_ret.copy()\n",
    "NN_deciles_log_ret.ret = np.log(1 + NN_deciles_log_ret.ret)\n",
    "NN_deciles_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Log returns of 10-1 portfolio\n",
    "LR_10_1_log_ret = LR_10_1_ret.copy()\n",
    "LR_10_1_log_ret.ret = np.log(1 + LR_10_1_log_ret.ret)\n",
    "LR_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "lasso_10_1_log_ret = lasso_10_1_ret.copy()\n",
    "lasso_10_1_log_ret.ret = np.log(1 + lasso_10_1_log_ret.ret)\n",
    "lasso_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "NN_10_1_log_ret = NN_10_1_ret.copy()\n",
    "NN_10_1_log_ret.ret = np.log(1 + NN_10_1_log_ret.ret)\n",
    "NN_10_1_log_ret.rename(columns = {\"ret\":\"log_ret\"}, inplace = True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Accumualtive log returns and predicted returns \n",
    "# of all deciles (cannot accumulate returns in pct.)\n",
    "LR_cum_log_ret = pred.portfolio_sorts_acc_return(LR_deciles_log_ret)\n",
    "lasso_cum_log_ret = pred.portfolio_sorts_acc_return(lasso_deciles_log_ret)\n",
    "NN_cum_log_ret = pred.portfolio_sorts_acc_return(NN_deciles_log_ret)\n",
    "\n",
    "# Monthly average return and std. deviation, and annualized SR\n",
    "# of both predicted and actual returns\n",
    "LR_mean, LR_std, LR_sr = pred.portfolio_sorts_SR(LR_deciles_ret)\n",
    "lasso_mean, lasso_std, lasso_sr = pred.portfolio_sorts_SR(lasso_deciles_ret)\n",
    "NN_mean, NN_std, NN_sr = pred.portfolio_sorts_SR(NN_deciles_ret)\n",
    "\n",
    "# Monthly average return and std. deviation, and annualized SR \n",
    "# for 10-1 portfolio\n",
    "LR_10_1_mean, LR_10_1_sd, LR_10_1_sr = pred.portfolio_sorts_SR(LR_10_1_ret)\n",
    "lasso_10_1_mean, lasso_10_1_sd, lasso_10_1_sr = pred.portfolio_sorts_SR(lasso_10_1_ret)\n",
    "NN_10_1_mean, NN_10_1_sd, NN_10_1_sr = pred.portfolio_sorts_SR(NN_10_1_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2165937-8930-4094-b3c3-d3d4c2d02f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of all deciles for each ML model\n",
    "pred.cumulative_ret_fig(data = LR_deciles_log_ret, name = \"LR_cumulative_log_ret\", market = SP500, save_fig = True, hide = True)\n",
    "pred.cumulative_ret_fig(data = lasso_deciles_log_ret, name = \"lasso_cumulative_log_ret\", market = SP500, save_fig = True, hide = True)\n",
    "pred.cumulative_ret_fig(data = NN_deciles_log_ret, name = \"NN_cumulative_log_ret\", market = SP500, save_fig = True, hide = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b1fa70c0-0912-4900-aa9b-a92889f5092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of 10-1 portfolio for each ML model\n",
    "pred.portfolio_10_1_fig(name = \"10-1_portfolio_cumulative_log_ret\", market = SP500, save_fig = True, hide = True, data1 = LR_10_1_log_ret, data2 = lasso_10_1_log_ret, data3 = NN_10_1_log_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "adc7fff4-1c51-49db-b767-eae67bb7437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of 1st and 10th decile of specified ML models \n",
    "pred.deciles_10_1_fig(name = \"deciles_10_1_cumulative_log_ret\", market = SP500, save_fig = True, hide = True, data1 = LR_deciles_log_ret, data2 = lasso_deciles_log_ret, data3 = NN_deciles_log_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ea367d1-d611-4efc-b97e-e854d14dcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for export to R so as to customize for tables \n",
    "\n",
    "# Table 1: monthly loss and explained variation (Multiply with 100 to get pct.)\n",
    "monthly_pricing_error = loss.mean(axis = 1) * 100\n",
    "monthly_xplained_variation = xplained_variation.mean(axis = 1) * 100\n",
    "table1 = pd.concat([monthly_pricing_error, monthly_xplained_variation], axis = 1)\n",
    "table1.columns = [\"Squared Pricing Error\", \"Explained Variation\"]\n",
    "table1 = table1.transpose()\n",
    "table1.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table1.to_csv(os.path.dirname(os.getcwd()) + '\\\\table1_data.csv', header = True, index = True)\n",
    "\n",
    "''' Incorrect as is. Consult Predictionstep1.py, to_append function\n",
    "# Table 2: Annual loss and explained variation (Multiply with 100 to get pct.)\n",
    "annual_pricing_error = loss_annual.mean(axis = 1) * 100\n",
    "annual_xplained_variation = xplained_variation_annual.mean(axis = 1) * 100\n",
    "table2 = pd.concat([annual_pricing_error, annual_xplained_variation], axis = 1)\n",
    "table2.columns = [\"Squared Pricing Error\", \"Explained Variation\"]\n",
    "table2 = table2.transpose()\n",
    "table2.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table2.to_csv(os.path.dirname(os.getcwd()) + '\\\\table2_data.csv', header = True, index = True)\n",
    "'''\n",
    "\n",
    "# Table 3: (Multiply with 100 to get pct.)\n",
    "table3_LR = pd.concat([LR_mean * 100, LR_std.ret * 100, LR_sr.ret], axis = 1).round(2)\n",
    "table3_LR.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_lasso = pd.concat([lasso_mean * 100 , lasso_std.ret * 100, lasso_sr.ret], axis = 1).round(2)\n",
    "table3_lasso.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_NN = pd.concat([NN_mean * 100, NN_std.ret * 100, NN_sr.ret], axis = 1).round(2)\n",
    "table3_NN.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "\n",
    "# Table 3: Add 10-1 portfolio row for each ML model\n",
    "LR_portfolio_10_1_row = pd.concat((LR_10_1_mean * 100, LR_10_1_sd.ret * 100, LR_10_1_sr.ret), axis = 1)\n",
    "LR_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_LR = pd.concat((table3_LR, LR_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "lasso_portfolio_10_1_row = pd.concat((lasso_10_1_mean * 100, lasso_10_1_sd.ret * 100, lasso_10_1_sr.ret), axis = 1)\n",
    "lasso_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_lasso = pd.concat((table3_lasso, lasso_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "NN_portfolio_10_1_row = pd.concat((NN_10_1_mean * 100, NN_10_1_sd.ret * 100, NN_10_1_sr.ret), axis = 1)\n",
    "NN_portfolio_10_1_row.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_NN = pd.concat((table3_NN, NN_portfolio_10_1_row), axis = 0).round(2)\n",
    "\n",
    "# Table 4: Tabel for appendix figurerne (skal bruge LR_cum, lasso_cum, NN_cum) og så bare kun ret søjlen. Har så tabel der viser end point for figurene (denne tabel kommer i appendix) \n",
    "table4 = pd.concat([LR_cum_log_ret.log_ret, lasso_cum_log_ret.log_ret, NN_cum_log_ret.log_ret], axis = 1).round(3)\n",
    "table4.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table4 = table4.transpose()\n",
    "table4.to_csv(os.path.dirname(os.getcwd()) + '\\\\table4_data.csv', header = True, index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec975aca-900c-4d04-98ac-f97aa08e01c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data for CAPM, FF3, and FF5 \n",
    "FF5 = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\F-F_Research_Data_5_Factors_2x3.csv')\n",
    "FF5.rename(columns = {'Unnamed: 0':'date'}, inplace = True)\n",
    "FF5 = FF5[(FF5[\"date\"] >= 198701) & (FF5[\"date\"] < 201612)]\n",
    "FF5 = FF5.set_index('date')\n",
    "FF5 = FF5[[\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]]\n",
    "\n",
    "# Define FF3 and CAPM from FF5\n",
    "FF3 = FF5[[\"Mkt-RF\", \"SMB\", \"HML\"]]\n",
    "CAPM = FF5[\"Mkt-RF\"]\n",
    "\n",
    "# LR regressions\n",
    "CAPM_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_LR = pred.linearmodel(LR_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# Lasso regressions\n",
    "CAPM_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_lasso = pred.linearmodel(lasso_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# NN regressions\n",
    "CAPM_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, CAPM)\n",
    "FF3_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, FF3)\n",
    "FF5_NN = pred.linearmodel(NN_10_1_ret[\"ret\"] * 100, FF5)\n",
    "\n",
    "# CAPM table\n",
    "CAPM_params, CAPM_tvalues = pred.LM_tables(CAPM_LR, CAPM_lasso, CAPM_NN)\n",
    "CAPM_params.index = [\"Constant\", \"Mkt-RF\"]\n",
    "CAPM_tvalues.index = [\"Constant\", \"Mkt-RF\"]\n",
    "CAPM_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\CAPM_params.csv', header = True, index = True) \n",
    "CAPM_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\CAPM_tvalues.csv', header = True, index = True) \n",
    "\n",
    "# FF3 table\n",
    "FF3_params, FF3_tvalues = pred.LM_tables(FF3_LR, FF3_lasso, FF3_NN)\n",
    "FF3_params.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\"]\n",
    "FF3_tvalues.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\"]\n",
    "FF3_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF3_params.csv', header = True, index = True) \n",
    "FF3_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF3_tvalues.csv', header = True, index = True) \n",
    "\n",
    "# FF5 table \n",
    "FF5_params, FF5_tvalues = pred.LM_tables(FF5_LR, FF5_lasso, FF5_NN)\n",
    "FF5_params.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "FF5_tvalues.index = [\"Constant\", \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "FF5_params.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF5_params.csv', header = True, index = True) \n",
    "FF5_tvalues.T.to_csv(os.path.dirname(os.getcwd()) + '\\\\FF5_tvalues.csv', header = True, index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bbef2e10-33fd-4105-a960-6c5932978af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    ret   R-squared:                       0.004\n",
      "Model:                            OLS   Adj. R-squared:                 -0.004\n",
      "Method:                 Least Squares   F-statistic:                    0.5078\n",
      "Date:                Thu, 08 Dec 2022   Prob (F-statistic):              0.677\n",
      "Time:                        10:13:40   Log-Likelihood:                -1228.0\n",
      "No. Observations:                 359   AIC:                             2464.\n",
      "Df Residuals:                     355   BIC:                             2480.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.9581      0.399      4.902      0.000       1.173       2.744\n",
      "x1            -0.0230      0.092     -0.249      0.803      -0.205       0.159\n",
      "x2            -0.1277      0.133     -0.960      0.337      -0.389       0.134\n",
      "x3             0.0611      0.137      0.446      0.656      -0.208       0.330\n",
      "==============================================================================\n",
      "Omnibus:                       92.153   Durbin-Watson:                   1.949\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3153.618\n",
      "Skew:                          -0.121   Prob(JB):                         0.00\n",
      "Kurtosis:                      17.518   Cond. No.                         4.72\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(FF3_LR.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "id": "2494feea-0765-460b-9e49-2ed2c1d3e2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    ret   R-squared:                       0.083\n",
      "Model:                            OLS   Adj. R-squared:                  0.068\n",
      "Method:                 Least Squares   F-statistic:                     5.636\n",
      "Date:                Sat, 03 Dec 2022   Prob (F-statistic):            0.00102\n",
      "Time:                        19:30:58   Log-Likelihood:                -620.64\n",
      "No. Observations:                 192   AIC:                             1249.\n",
      "Df Residuals:                     188   BIC:                             1262.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.0186      0.459      4.402      0.000       1.114       2.923\n",
      "x1             0.3403      0.108      3.148      0.002       0.127       0.553\n",
      "x2             0.0995      0.134      0.743      0.458      -0.165       0.364\n",
      "x3             0.6140      0.161      3.815      0.000       0.296       0.931\n",
      "==============================================================================\n",
      "Omnibus:                       55.293   Durbin-Watson:                   1.905\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1352.794\n",
      "Skew:                           0.261   Prob(JB):                    1.76e-294\n",
      "Kurtosis:                      15.993   Cond. No.                         5.33\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(FF3_LR.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78618e90-443a-4ca2-9a69-914ee0e2f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skal jeg lave en normals OLS på ikke PCA data? bare som totalt benchmark? -- køre det på FM_data + industry codes så -- ingen interaktion terms eller PCA "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
