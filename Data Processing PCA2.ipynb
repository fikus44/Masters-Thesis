{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "8de3de21-6556-4f1e-80e2-80a89e701949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load dependencies\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import itertools as it\n",
    "import DataProcessFunctions as DP\n",
    "import PredictionStep1 as pred\n",
    "import SupportFunctions as supp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from tensorflow.keras.layers import Dense\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8f707-f8b3-4490-85d4-9463d4525581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize warning log container \n",
    "log = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76be5616-1015-4f3d-b1bc-de5deed9e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before downcast: 1.769 GB and float64    102\n",
      "int64        3\n",
      "dtype: int64\n",
      "After downcast: 0.878 GB and float32    102\n",
      "int32        2\n",
      "int8         1\n",
      "dtype: int64\n",
      "Before downcast: 0.026 GB and float64    1\n",
      "dtype: int64\n",
      "After downcast: 0.018 GB and float32    1\n",
      "dtype: int64\n",
      "Before downcast: 0.026 GB and float64    1\n",
      "dtype: int64\n",
      "After downcast: 0.018 GB and float32    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load Pre-processed data from Data Processing PCA.ipynb\n",
    "FM_data = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\FM2_data.csv')\n",
    "returns = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\returns2_data.csv').set_index([\"permno\", \"date\"])\n",
    "industry_code = pd.read_csv(os.path.dirname(os.getcwd()) + '\\\\industry2_codes.csv').set_index([\"permno\", \"date\"])\n",
    "\n",
    "supp.downcast(FM_data)\n",
    "supp.downcast(returns)\n",
    "supp.downcast(industry_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa87af2c-5685-4f1c-a59f-1cbcb4e35d77",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set number of iterations\n",
    "ite = 29 # 30 splits --> iteration = [0;29]\n",
    "\n",
    "# Initialize arrays to store loss and explained variation\n",
    "loss = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "xplained_variation = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "\n",
    "# Initialize arrays to store annual loss and explained variation\n",
    "loss_annual = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "xplained_variation_annual = pd.DataFrame(columns = range(ite), index = [\"LR\", \"Lasso\", \"NN\"])\n",
    "\n",
    "# Initialize arrays to store predicted and actual returns used in portfolio sorts later\n",
    "LR_pred_actual = pd.DataFrame()\n",
    "lasso_pred_actual = pd.DataFrame()\n",
    "NN_pred_actual = pd.DataFrame()\n",
    "\n",
    "for i in range(ite):\n",
    "    \n",
    "    # Compute training, validation, and test set\n",
    "    training, validation, test = DP.complete_data_process(industry_code, returns, FM_data, iteration = i)\n",
    "    \n",
    "    # Split in X and Y\n",
    "    training_x, training_y = pred.XY_split(training)\n",
    "    validation_x, validation_y = pred.XY_split(validation)\n",
    "    test_x, test_y = pred.XY_split(test)\n",
    "    \n",
    "    # Algorithm 1: Simple Linear (PCR in Gu, kelly, and Xiu (2020) due to PCA)\n",
    "    # Fit model on training set & predict on test set\n",
    "    LR = lr().fit(training_x, training_y)\n",
    "    LR_pred = LR.predict(test_x)\n",
    "    \n",
    "    # Algoritm 1: Compute loss and explained varation, and combine \n",
    "    # actual and predicted returns. Append all. \n",
    "    LR_loss, LR_explained_var, LR_pred_actual_temp, LR_loss_annual, LR_explained_var_annual = pred.to_append(LR_pred, test_y)\n",
    "    loss.iloc[0, i] = LR_loss\n",
    "    loss_annual.iloc[0, i] = LR_loss_annual\n",
    "    xplained_variation.iloc[0, i] = LR_explained_var\n",
    "    xplained_variation_annual.iloc[0, i] = LR_explained_var_annual\n",
    "    LR_pred_actual = LR_pred_actual.append(LR_pred_actual_temp)\n",
    "    \n",
    "    \n",
    "    # ALgorithm 2: LASSO\n",
    "    # Fit model on training set and select tuning parameter based on validation set\n",
    "    lambda_grid = pred.lambda_grid(training_x, training_y)\n",
    "    loss_validation = []\n",
    "\n",
    "    for lamb in lambda_grid:\n",
    "        lasso = Lasso(alpha = lamb, tol = 0.001).fit(training_x, training_y)\n",
    "        lasso_pred = lasso.predict(validation_x)\n",
    "        loss_validation.append(pred.loss_function(lasso_pred, validation_y))\n",
    "\n",
    "    # Fit model with error minimizing tuning parameter\n",
    "    lambda_min = lambda_grid[loss_validation.index(min(loss_validation))]\n",
    "    lasso_min = Lasso(alpha = lambda_min).fit(training_x, training_y)\n",
    "    lasso_min_pred = lasso_min.predict(test_x)\n",
    "\n",
    "    # Algorithm 2: Appending \n",
    "    lasso_loss, lasso_explained_var, lasso_pred_actual_temp, lasso_loss_annual, lasso_explained_var_annual = pred.to_append(lasso_min_pred, test_y)\n",
    "    loss.iloc[1, i] = lasso_loss\n",
    "    loss_annual.iloc[1, i] = lasso_loss_annual\n",
    "    xplained_variation.iloc[1, i] = lasso_explained_var\n",
    "    xplained_variation_annual.iloc[1, i] = lasso_explained_var_annual\n",
    "    lasso_pred_actual = lasso_pred_actual.append(lasso_pred_actual_temp)\n",
    "    \n",
    "    \n",
    "    # ALgorithm 3: NN \n",
    "    # Arkitektur skal gøres uden for loopet! Skal jeg gøre NN mere avanceret? Jeg har stadig linear som de sidste lag også \n",
    "    # Build NN architecture \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(Dense(units = 32, activation = 'relu', input_dim = len(training_x.columns))) # 1. Hidden layer\n",
    "    model.add(Dense(units = 16, activation = 'relu')) # 2. Hidden layer\n",
    "    model.add(Dense(units = 8, activation = 'relu')) # 3. Hidden layer\n",
    "    model.add(Dense(units = 4, activation = 'relu')) # 4. Hidden layer\n",
    "    model.add(Dense(units = 1, activation = 'linear')) # Output layer - linear fixes nonnegative predictions\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error') # jeg kan også bruge huber loss ved 'huber'\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(training_x, training_y, epochs = 1, batch_size = 40, verbose = 1)\n",
    "\n",
    "    # Compute predictions\n",
    "    NN_pred = model.predict(test_x)\n",
    "    \n",
    "    # Appending\n",
    "    NN_loss, NN_explained_var, NN_pred_actual_temp, NN_loss_annual, NN_explained_var_annual = pred.to_append(NN_pred, test_y)\n",
    "    loss.iloc[2, i] = NN_loss\n",
    "    loss_annual.iloc[2, i] = NN_loss_annual\n",
    "    xplained_variation.iloc[2, i] = NN_explained_var\n",
    "    xplained_variation_annual.iloc[2, i] = NN_explained_var_annual\n",
    "    NN_pred_actual = NN_pred_actual.append(NN_pred_actual_temp)\n",
    "    \n",
    "    del training\n",
    "    del validation\n",
    "    del test\n",
    "    del training_x\n",
    "    del training_y\n",
    "    del validation_x\n",
    "    del validation_y\n",
    "    del test_x\n",
    "    del test_y\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "dbb4a794-fcf2-4e31-ba2f-9ad89c67f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return for each decile at all points in time\n",
    "LR_step_1 = pred.portfolio_sorts_1(LR_pred_actual)\n",
    "lasso_step_1 = pred.portfolio_sorts_1(lasso_pred_actual)\n",
    "NN_step_1 = pred.portfolio_sorts_1(NN_pred_actual)\n",
    "\n",
    "# Accumualtive return of all deciles for both\n",
    "# predicted and actual returns \n",
    "LR_cum = pred.portfolio_sorts_acc_return(LR_step_1)\n",
    "lasso_cum = pred.portfolio_sorts_acc_return(lasso_step_1)\n",
    "NN_cum = pred.portfolio_sorts_acc_return(NN_step_1)\n",
    "\n",
    "# Monthly average return and std. deviation, and annualized SR\n",
    "# of both predicted and actual returns\n",
    "LR_mean, LR_std, LR_sr = pred.portfolio_sorts_SR(LR_step_1)\n",
    "lasso_mean, lasso_std, lasso_sr = pred.portfolio_sorts_SR(lasso_step_1)\n",
    "NN_mean, NN_std, NN_sr = pred.portfolio_sorts_SR(NN_step_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "a2165937-8930-4094-b3c3-d3d4c2d02f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of all deciles for each ML model\n",
    "pred.cumulative_ret_fig(data = LR_step_1, name = \"LR_cumulative_ret\", save_fig = True, hide = True)\n",
    "pred.cumulative_ret_fig(data = lasso_step_1, name = \"lasso_cumulative_ret\", save_fig = True, hide = True)\n",
    "pred.cumulative_ret_fig(data = NN_step_1, name = \"NN_cumulative_ret\", save_fig = True, hide = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "adc7fff4-1c51-49db-b767-eae67bb7437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Cumulative return of 1st and 10th decile of specified ML models \n",
    "pred.deciles_10_1_fig(name = \"deciles_10_1\", save_fig = True, hide = True, data1 = LR_step_1, data2 = lasso_step_1, data3 = NN_step_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "3ea367d1-d611-4efc-b97e-e854d14dcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for export to R so as to customaize for tables \n",
    "\n",
    "# Table 1: monthly loss and explained variation \n",
    "monthly_pricing_error = loss.mean(axis = 1)\n",
    "monthly_xplained_variation = xplained_variation.mean(axis = 1)\n",
    "table1 = pd.concat([monthly_pricing_error, monthly_xplained_variation], axis = 1)\n",
    "table1.columns = [\"Squared Pricing Error\", \"Explained Variation\"]\n",
    "table1 = table1.transpose()\n",
    "table1.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table1.to_csv(os.path.dirname(os.getcwd()) + '\\\\table1_data.csv', header = True, index = True)\n",
    "\n",
    "# Table 2: Annual loss and explained variation\n",
    "annual_pricing_error = loss_annual.mean(axis = 1)\n",
    "annual_xplained_variation = xplained_variation_annual.mean(axis = 1)\n",
    "table2 = pd.concat([annual_pricing_error, annual_xplained_variation], axis = 1)\n",
    "table2.columns = [\"Squared Pricing Error\", \"Explained Variation\"]\n",
    "table2 = table2.transpose()\n",
    "table2.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table2.to_csv(os.path.dirname(os.getcwd()) + '\\\\table2_data.csv', header = True, index = True)\n",
    "\n",
    "# Table 3:\n",
    "table3_LR = pd.concat([LR_mean, LR_std.ret, LR_sr.ret], axis = 1).round(3)\n",
    "table3_LR.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_lasso = pd.concat([lasso_mean, lasso_std.ret, lasso_sr.ret], axis = 1).round(3)\n",
    "table3_lasso.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "table3_NN = pd.concat([NN_mean, NN_std.ret, NN_sr.ret], axis = 1).round(3)\n",
    "table3_NN.columns = [\"Avg\", \"Pred\", \"Std\", \"SR\"]\n",
    "\n",
    "# Table 4: Tabel for appendix figurerne (skal bruge LR_cum, lasso_cum, NN_cum) og så bare kun ret søjlen. Har så tabel der viser end point for figurene (denne tabel kommer i appendix) \n",
    "table4 = pd.concat([LR_cum.ret, lasso_cum.ret, NN_cum.ret], axis = 1).round(3)\n",
    "table4.columns = [\"Linear Regression\", \"Lasso\", \"Neural Network\"]\n",
    "table4 = table4.transpose()\n",
    "table4.to_csv(os.path.dirname(os.getcwd()) + '\\\\table4_data.csv', header = True, index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "40f3f555-11d4-40e5-95ed-b5aca94c5342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>decile</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.646</td>\n",
       "      <td>1.095</td>\n",
       "      <td>1.349</td>\n",
       "      <td>1.738</td>\n",
       "      <td>1.890</td>\n",
       "      <td>1.948</td>\n",
       "      <td>2.122</td>\n",
       "      <td>2.896</td>\n",
       "      <td>4.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.257</td>\n",
       "      <td>0.589</td>\n",
       "      <td>1.153</td>\n",
       "      <td>1.284</td>\n",
       "      <td>1.723</td>\n",
       "      <td>2.057</td>\n",
       "      <td>2.123</td>\n",
       "      <td>2.595</td>\n",
       "      <td>2.722</td>\n",
       "      <td>3.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network</th>\n",
       "      <td>0.927</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.289</td>\n",
       "      <td>1.650</td>\n",
       "      <td>1.742</td>\n",
       "      <td>1.847</td>\n",
       "      <td>2.165</td>\n",
       "      <td>2.098</td>\n",
       "      <td>2.271</td>\n",
       "      <td>2.881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "decile                 1      2      3      4      5      6      7      8  \\\n",
       "Linear Regression -0.048  0.646  1.095  1.349  1.738  1.890  1.948  2.122   \n",
       "Lasso              0.257  0.589  1.153  1.284  1.723  2.057  2.123  2.595   \n",
       "Neural Network     0.927  1.380  1.289  1.650  1.742  1.847  2.165  2.098   \n",
       "\n",
       "decile                 9     10  \n",
       "Linear Regression  2.896  4.614  \n",
       "Lasso              2.722  3.748  \n",
       "Neural Network     2.271  2.881  "
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f90ab-3401-46de-b8be-5c91aa8353a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tror jeg skal dele portfolio sorts ind i 2 dele (2 funktinoer). Jeg har den første nu, som for hver periode hver decil spytter returns ud. Derfra skal jeg så bruge det output til 2 forskellige ting.\n",
    "# 1) Jeg skal akkumulere returns over perioden for hver decil, så jeg ender med at kumulativt return for hver decil, i.e. 10 tal\n",
    "# 2) For at lave figur A.7 i gu, kelly xiu appendix + for at regne sharpe ratio ud, så skal jeg bruge returns for hver decil i hver periode. For at lave figuren skal jeg til hver periode akkumulere returns a la hvad jeg \n",
    "# gjorde for PCA figuren. For at regne sharpe ratio ud skal jeg for hver decil over perioden finde average return og std. dev. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c038c71a-a330-4c8e-83f2-89d1c2842432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78618e90-443a-4ca2-9a69-914ee0e2f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio sorts; groupby dato. for hver gruppe splitter vi nu preds op i deciler og calculater return for hver portefølje (på det faktiske return). Vi gør dette for hver måned og så får vi så efter et år 12 observationer\n",
    "# for hver portefølje, vi kan nu calculate sharpe ratio for hver a porteføljerne (mobthly sharpe ratio) baseret på det ene år, hvis vi nu har længere periode kan jeg compoute monthly sharpe ratio på den længere periode, man kan så\n",
    "# annualize ved at gange med sqrt(12). \n",
    "\n",
    "# hvad siger gu kelly og xiu om portfolio sorts hvad gør de? \n",
    "\n",
    "# det kunne være sjovt at sammenligne med konventielle asset pricing modeller som f.eks. FF3 og CAPM -- gør de vel egentlig ogsåi  gu kelly og xiu \n",
    "\n",
    "\n",
    "# jeg skal have ryddet op i koden med loopet. både NN struktur men også bare resten generelt \n",
    "# Skal jeg lave en normals OLS på ikke PCA data? bare som totalt benchmark? \n",
    "\n",
    "# jeg skal lave gu kelly xiu tabel 7 -- er der andre tabeller jeg skal have med? Jeg tænker at eksportere relevant data (i korrekt format) til R, så jeg kan bruge table-theme jeg har lavet der"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
